{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sarcasm detection on twitter dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GauravSahani1417/Ap-Script-Hackathon-Brute-Force/blob/main/Sarcasm_detection_on_twitter_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23k3deCA4Aj"
      },
      "source": [
        "Mounted drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-fM21h5T_Tl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "23579e96-bdae-47e7-9291-9cab69d71dae"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6zltLAwBEbE"
      },
      "source": [
        "Preprocessing of data : Tokenization,lemmatization and expanding abbreviations\n",
        "\n",
        "1.   removing all usernames follwed by # tags\n",
        "2.   removing punctuations\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_56ss3DyAEQ"
      },
      "source": [
        "def remove_stopwords(x):\n",
        "  new_wordlist = [ ]\n",
        "  for i in x:\n",
        "    if i not in stopwords.words('english'):\n",
        "      new_wordlist.append(i)\n",
        "  return new_wordlist\n",
        "\n",
        "def word_tokenize(x):\n",
        "  return nltk.word_tokenize(str(x))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cutter(word):\n",
        "        if len(word) < 4:\n",
        "            return word\n",
        "        return wnl.lemmatize(wnl.lemmatize(word, \"n\"), \"v\")\n",
        "      \n",
        "def preprocess(string):\n",
        "        string = string.lower().replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"′\", \"'\").replace(\"’\", \"'\") \\\n",
        "            .replace(\"won't\", \"will not\").replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\") \\\n",
        "            .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\") \\\n",
        "            .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"i'd\", \"i would\").replace(\"'re\", \" are\") \\\n",
        "            .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\") \\\n",
        "            .replace(\"%\", \" percent \").replace(\"₹\", \" rupee \").replace(\"$\", \" dollar \") \\\n",
        "            .replace(\"€\", \" euro \").replace(\"'ll\", \" will\").replace(\"=\", \" equal \").replace(\"+\", \" plus \")\n",
        "    \n",
        "        string = re.sub(\"'ll\", \" will\", string)\n",
        "        string = re.sub(\"'ve\", \" have\", string)\n",
        "        string = re.sub(\"n't\", \" not\", string)\n",
        "        string = re.sub(\"'d\", \" would\", string)\n",
        "        string = re.sub(\"'re\", \" are\", string)\n",
        "        string = re.sub(\"i'm\", \"i am\", string)\n",
        "        string = re.sub(\"it's\", \"it is\", string)\n",
        "        string = re.sub(\"she's\", \"she is\", string)\n",
        "        string = re.sub(\"he's\", \"he is\", string)\n",
        "        string = re.sub(\"here's\", \"here is\", string)\n",
        "        string = re.sub(\"that's\", \"that is\", string)\n",
        "        string = re.sub(\"there's\", \"there is\", string)\n",
        "        string = re.sub(\"what's\", \"what is\", string)\n",
        "        string = re.sub(\"who's\", \"who is\", string)\n",
        "        string = re.sub(\"'s\", \"\", string)\n",
        "       \n",
        "        string = re.sub(r\"\\btmrw\\b\", \"tomorrow\", string)\n",
        "        string = re.sub(r\"\\bur\\b\", \"your\", string)\n",
        "        string = re.sub(r\"\\burs\\b\", \"yours\", string)\n",
        "        string = re.sub(r\"\\bppl\\b\", \"people\", string)\n",
        "        string = re.sub(r\"\\byrs\\b\", \"years\", string)\n",
        "     \n",
        "        string = re.sub(r\"\\b(rt)\\b\", \"retweet\", string)\n",
        "        string = re.sub(r\"\\b(btw)\\b\", \"by the way\", string)\n",
        "        string = re.sub(r\"\\b(asap)\\b\", \"as soon as possible\", string)\n",
        "        string = re.sub(r\"\\b(fyi)\\b\", \"for your information\", string)\n",
        "        string = re.sub(r\"\\b(tbt)\\b\", \"throwback thursday\", string)\n",
        "        string = re.sub(r\"\\b(tba)\\b\", \"to be announced\", string)\n",
        "        string = re.sub(r\"\\b(tbh)\\b\", \"to be honest\", string)\n",
        "        string = re.sub(r\"\\b(faq)\\b\", \"frequently asked questions\", string)\n",
        "        string = re.sub(r\"\\b(icymi)\\b\", \"in case you missed it\", string)\n",
        "        string = re.sub(r\"\\b(aka)\\b\", \"also known as\", string)\n",
        "        string = re.sub(r\"\\b(ama)\\b\", \"ask me anything\", string)\n",
        "        \n",
        "       \n",
        "        string = re.sub(r\"http\\S+\", \"\", string)\n",
        "      \n",
        "        string = re.sub(\"@[^\\s]*\", \"\", string)\n",
        "      \n",
        "        string = re.sub(\"#[^\\s]*\", \"\", string)\n",
        "        \n",
        "        string = re.sub('[“”\\(\\'…\\)\\!\\^\\\"\\.;:,\\-\\?？\\{\\}\\[\\]\\\\/\\*@]', ' ', string)\n",
        "        string = re.sub(r\"([0-9]+)000000\", r\"\\1m\", string)\n",
        "        string = re.sub(r\"([0-9]+)000\", r\"\\1k\", string)\n",
        "        string = html.unescape(string)\n",
        "        string = re.sub(\"[^a-zA-Z]\",\" \",string)\n",
        "        string = ' '.join([cutter(w) for w in string.split()])\n",
        "        return string\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXsZa_l1CSTP"
      },
      "source": [
        "Importing various libraries and \n",
        "organizing my dataframe for furthur analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRiCIuCi-T_m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "6b3acff7-eae0-42a0-8713-0f4fe52f13d4"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import html\n",
        "\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import pos_tag\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from collections import defaultdict\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Embedding\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "stemmer=LancasterStemmer()\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger') \n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "df=pd.read_csv('drive/My Drive/Colab Notebooks/datasar.txt',sep='\\t',header=None)\n",
        "\n",
        "df = df[[1,2]]\n",
        "df.columns = ['label','text']\n",
        "df['text'] = df['text'].apply(lambda x: \" \".join(['XYZ' if p.startswith('@')==True or p.startswith('#')==True else p for p in x.split()]))\n",
        "df['label'] = df['label'].apply(lambda x: 1 if x == 4 else 0)\n",
        "\n",
        "df['text'] = df['text'].apply(preprocess)\n",
        "text1=df['text']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rTLv2N3C-gC"
      },
      "source": [
        "A column of Tokenized words in dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOv6Zr_r2n3p"
      },
      "source": [
        "df['w_tokenized'] = df['text'].apply(lambda x: word_tokenize(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-2WMyC_3OWj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "90485b47-1595-4874-864a-b743df3336a9"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>w_tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>xyz i hope youre lurk rn i want to listen to h...</td>\n",
              "      <td>[xyz, i, hope, youre, lurk, rn, i, want, to, l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>really teach me a valuable lesson i am never g...</td>\n",
              "      <td>[really, teach, me, a, valuable, lesson, i, am...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>xyz never had a voice to protest so you fed me...</td>\n",
              "      <td>[xyz, never, had, a, voice, to, protest, so, y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>xyz rest in peace love to you and your family</td>\n",
              "      <td>[xyz, rest, in, peace, love, to, you, and, you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>day until christmas xyz soon xyz ready yet</td>\n",
              "      <td>[day, until, christmas, xyz, soon, xyz, ready,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                        w_tokenized\n",
              "0      0  ...  [xyz, i, hope, youre, lurk, rn, i, want, to, l...\n",
              "1      0  ...  [really, teach, me, a, valuable, lesson, i, am...\n",
              "2      0  ...  [xyz, never, had, a, voice, to, protest, so, y...\n",
              "3      0  ...  [xyz, rest, in, peace, love, to, you, and, you...\n",
              "4      0  ...  [day, until, christmas, xyz, soon, xyz, ready,...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCt-m_HwDdZW"
      },
      "source": [
        "(part of speech tagging)\n",
        "tagging all tokenized words and mapping it grammatically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxImEp_E65u7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "5018953f-b8a0-4a6c-f623-812ecd8076ce"
      },
      "source": [
        "import string\n",
        "def pos_tagsa(x):\n",
        "  return nltk.pos_tag(x)\n",
        "df['pos'] = df['w_tokenized'].apply(lambda x: pos_tagsa(x))\n",
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>w_tokenized</th>\n",
              "      <th>pos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>xyz i hope youre lurk rn i want to listen to h...</td>\n",
              "      <td>[xyz, i, hope, youre, lurk, rn, i, want, to, l...</td>\n",
              "      <td>[(xyz, NN), (i, NNS), (hope, VBP), (youre, NN)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>really teach me a valuable lesson i am never g...</td>\n",
              "      <td>[really, teach, me, a, valuable, lesson, i, am...</td>\n",
              "      <td>[(really, RB), (teach, VB), (me, PRP), (a, DT)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>xyz never had a voice to protest so you fed me...</td>\n",
              "      <td>[xyz, never, had, a, voice, to, protest, so, y...</td>\n",
              "      <td>[(xyz, NN), (never, RB), (had, VBD), (a, DT), ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>xyz rest in peace love to you and your family</td>\n",
              "      <td>[xyz, rest, in, peace, love, to, you, and, you...</td>\n",
              "      <td>[(xyz, JJ), (rest, NN), (in, IN), (peace, NN),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>day until christmas xyz soon xyz ready yet</td>\n",
              "      <td>[day, until, christmas, xyz, soon, xyz, ready,...</td>\n",
              "      <td>[(day, NN), (until, IN), (christmas, JJ), (xyz...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ...                                                pos\n",
              "0      0  ...  [(xyz, NN), (i, NNS), (hope, VBP), (youre, NN)...\n",
              "1      0  ...  [(really, RB), (teach, VB), (me, PRP), (a, DT)...\n",
              "2      0  ...  [(xyz, NN), (never, RB), (had, VBD), (a, DT), ...\n",
              "3      0  ...  [(xyz, JJ), (rest, NN), (in, IN), (peace, NN),...\n",
              "4      0  ...  [(day, NN), (until, IN), (christmas, JJ), (xyz...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHGZdlHBElWA"
      },
      "source": [
        "creating features such as word count,char count,(lengths of comment) and average word length for every comment.These features later are used for analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lhqf8HEUTKQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "a97d35f3-8efd-42d0-c0e6-3afd887b1ddf"
      },
      "source": [
        "df['word_count'] = df['w_tokenized'].apply(lambda x: len(str(x).split(\" \")))\n",
        "#df[['lemma','word_count']].head()  \n",
        "df['char_count'] = df['w_tokenized'].str.len()\n",
        "#df[['lemma','char_count']].head()\n",
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>w_tokenized</th>\n",
              "      <th>pos</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>xyz i hope youre lurk rn i want to listen to h...</td>\n",
              "      <td>[xyz, i, hope, youre, lurk, rn, i, want, to, l...</td>\n",
              "      <td>[(xyz, NN), (i, NNS), (hope, VBP), (youre, NN)...</td>\n",
              "      <td>21</td>\n",
              "      <td>21</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>really teach me a valuable lesson i am never g...</td>\n",
              "      <td>[really, teach, me, a, valuable, lesson, i, am...</td>\n",
              "      <td>[(really, RB), (teach, VB), (me, PRP), (a, DT)...</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>xyz never had a voice to protest so you fed me...</td>\n",
              "      <td>[xyz, never, had, a, voice, to, protest, so, y...</td>\n",
              "      <td>[(xyz, NN), (never, RB), (had, VBD), (a, DT), ...</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>xyz rest in peace love to you and your family</td>\n",
              "      <td>[xyz, rest, in, peace, love, to, you, and, you...</td>\n",
              "      <td>[(xyz, JJ), (rest, NN), (in, IN), (peace, NN),...</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>day until christmas xyz soon xyz ready yet</td>\n",
              "      <td>[day, until, christmas, xyz, soon, xyz, ready,...</td>\n",
              "      <td>[(day, NN), (until, IN), (christmas, JJ), (xyz...</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  ... char_count\n",
              "0      0  ...         21\n",
              "1      0  ...         15\n",
              "2      0  ...         25\n",
              "3      0  ...         10\n",
              "4      0  ...          8\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iQ2mNPAYSLN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2640233c-9ef8-4f6b-d397-d1e679bcddad"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['label', 'text', 'w_tokenized', 'pos', 'word_count', 'char_count'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r86mdF0zn_ll",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "0f2a8b9f-1151-406e-85c3-46cb673cc30c"
      },
      "source": [
        "df['lengths'] = (df['text'].str.split()).apply(len)\n",
        "df = df[df['lengths']>7]\n",
        "df.reset_index(inplace=True)\n",
        "\n",
        "df = df.sample(frac=1)\n",
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>w_tokenized</th>\n",
              "      <th>pos</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>lengths</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31229</th>\n",
              "      <td>34326</td>\n",
              "      <td>0</td>\n",
              "      <td>retweet its okay with me when youtube put ads ...</td>\n",
              "      <td>[retweet, its, okay, with, me, when, youtube, ...</td>\n",
              "      <td>[(retweet, VB), (its, PRP$), (okay, NN), (with...</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28057</th>\n",
              "      <td>31025</td>\n",
              "      <td>0</td>\n",
              "      <td>xyz well you are friendly this morning xyz xyz</td>\n",
              "      <td>[xyz, well, you, are, friendly, this, morning,...</td>\n",
              "      <td>[(xyz, RB), (well, RB), (you, PRP), (are, VBP)...</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5063</th>\n",
              "      <td>5667</td>\n",
              "      <td>0</td>\n",
              "      <td>happy birthday to bestfriend of year i love yo...</td>\n",
              "      <td>[happy, birthday, to, bestfriend, of, year, i,...</td>\n",
              "      <td>[(happy, JJ), (birthday, NN), (to, TO), (bestf...</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23730</th>\n",
              "      <td>26463</td>\n",
              "      <td>0</td>\n",
              "      <td>if someone want to help me study for the ap st...</td>\n",
              "      <td>[if, someone, want, to, help, me, study, for, ...</td>\n",
              "      <td>[(if, IN), (someone, NN), (want, VBP), (to, TO...</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35958</th>\n",
              "      <td>39290</td>\n",
              "      <td>0</td>\n",
              "      <td>yay for miss the westfield game this weekend x...</td>\n",
              "      <td>[yay, for, miss, the, westfield, game, this, w...</td>\n",
              "      <td>[(yay, NN), (for, IN), (miss, WP), (the, DT), ...</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       index  label  ... char_count lengths\n",
              "31229  34326      0  ...         22      22\n",
              "28057  31025      0  ...          9       9\n",
              "5063    5667      0  ...         18      18\n",
              "23730  26463      0  ...         18      18\n",
              "35958  39290      0  ...         10      10\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l6QajaXFL78"
      },
      "source": [
        "statistical data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AIt76lwYeq1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "ccdb7fcf-4fb2-4022-d5ee-51a4df64a8e6"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>lengths</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>36423.000000</td>\n",
              "      <td>36423.0</td>\n",
              "      <td>36423.000000</td>\n",
              "      <td>36423.000000</td>\n",
              "      <td>36423.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>20217.914999</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.648107</td>\n",
              "      <td>17.648107</td>\n",
              "      <td>17.618483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>11523.048442</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.887835</td>\n",
              "      <td>5.887835</td>\n",
              "      <td>5.876501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>8.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>10168.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>13.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>20548.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>17.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>30200.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>22.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>39779.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>45.000000</td>\n",
              "      <td>45.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              index    label    word_count    char_count       lengths\n",
              "count  36423.000000  36423.0  36423.000000  36423.000000  36423.000000\n",
              "mean   20217.914999      0.0     17.648107     17.648107     17.618483\n",
              "std    11523.048442      0.0      5.887835      5.887835      5.876501\n",
              "min        0.000000      0.0      8.000000      8.000000      8.000000\n",
              "25%    10168.500000      0.0     13.000000     13.000000     13.000000\n",
              "50%    20548.000000      0.0     17.000000     17.000000     17.000000\n",
              "75%    30200.500000      0.0     22.000000     22.000000     22.000000\n",
              "max    39779.000000      0.0     45.000000     45.000000     45.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpajKm2hUvrN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "d438f119-1253-44e9-f93b-8d772aedc970"
      },
      "source": [
        "\n",
        "def avg_word(sentence):\n",
        "  words = sentence\n",
        "  return (sum(len(word) for word in words)/(len(words) + 0.001))\n",
        "\n",
        "df['avg_word'] = df['w_tokenized'].apply(lambda x: avg_word(x))\n",
        "#df[['w_tokenized','avg_word']].head()\n",
        "df.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "      <th>w_tokenized</th>\n",
              "      <th>pos</th>\n",
              "      <th>word_count</th>\n",
              "      <th>char_count</th>\n",
              "      <th>lengths</th>\n",
              "      <th>avg_word</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31229</th>\n",
              "      <td>34326</td>\n",
              "      <td>0</td>\n",
              "      <td>retweet its okay with me when youtube put ads ...</td>\n",
              "      <td>[retweet, its, okay, with, me, when, youtube, ...</td>\n",
              "      <td>[(retweet, VB), (its, PRP$), (okay, NN), (with...</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>4.227081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28057</th>\n",
              "      <td>31025</td>\n",
              "      <td>0</td>\n",
              "      <td>xyz well you are friendly this morning xyz xyz</td>\n",
              "      <td>[xyz, well, you, are, friendly, this, morning,...</td>\n",
              "      <td>[(xyz, RB), (well, RB), (you, PRP), (are, VBP)...</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>4.221753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5063</th>\n",
              "      <td>5667</td>\n",
              "      <td>0</td>\n",
              "      <td>happy birthday to bestfriend of year i love yo...</td>\n",
              "      <td>[happy, birthday, to, bestfriend, of, year, i,...</td>\n",
              "      <td>[(happy, JJ), (birthday, NN), (to, TO), (bestf...</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>4.610855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23730</th>\n",
              "      <td>26463</td>\n",
              "      <td>0</td>\n",
              "      <td>if someone want to help me study for the ap st...</td>\n",
              "      <td>[if, someone, want, to, help, me, study, for, ...</td>\n",
              "      <td>[(if, IN), (someone, NN), (want, VBP), (to, TO...</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>18</td>\n",
              "      <td>3.777568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35958</th>\n",
              "      <td>39290</td>\n",
              "      <td>0</td>\n",
              "      <td>yay for miss the westfield game this weekend x...</td>\n",
              "      <td>[yay, for, miss, the, westfield, game, this, w...</td>\n",
              "      <td>[(yay, NN), (for, IN), (miss, WP), (the, DT), ...</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>4.299570</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       index  label  ... lengths  avg_word\n",
              "31229  34326      0  ...      22  4.227081\n",
              "28057  31025      0  ...       9  4.221753\n",
              "5063    5667      0  ...      18  4.610855\n",
              "23730  26463      0  ...      18  3.777568\n",
              "35958  39290      0  ...      10  4.299570\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6w4Q4oppeqh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cfef05f2-fe8d-4a15-b188-4a4f00a5d167"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36423, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRt_ME2bp1m8"
      },
      "source": [
        "MIN_WORD_OCCURRENCE = 50\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "REPLACE_WORD = 'XYZ'\n",
        "EMBEDDING_DIM = 300\n",
        "EMBEDDING_FILE = \"drive/My Drive/Colab Notebooks/glove_embeddings.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvhJQvfiGt9o"
      },
      "source": [
        "Eliminating all those texts which dont have label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XedlYyHMzQKz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f687b0df-ac8d-4d93-af43-1df189cdb216"
      },
      "source": [
        "df=df.sort_index(ascending=True)  ## to make equal number of label and text,there exists 3357 rows of text which dont have label\n",
        "print(df[['label','text']])\n",
        "print(text1)\n",
        "print(\"/////////\")\n",
        "for i in df['label']:\n",
        "  text2=df['text']\n",
        "print(text2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       label                                               text\n",
            "0          0  xyz i hope youre lurk rn i want to listen to h...\n",
            "1          0  really teach me a valuable lesson i am never g...\n",
            "2          0  xyz never had a voice to protest so you fed me...\n",
            "3          0      xyz rest in peace love to you and your family\n",
            "4          0         day until christmas xyz soon xyz ready yet\n",
            "5          0  xyz xyz yay can not wait to be reunite with yo...\n",
            "6          0  word short of the word requirement but i do no...\n",
            "7          0  xyz it was nice hang out this afternoon i had ...\n",
            "8          0          k walk this morning we did an awesome job\n",
            "9          0  time one direction had the perfect love song f...\n",
            "10         0  xyz yet again if you list them on twitter you ...\n",
            "11         0          xyz wow sweet butt for all of them i love\n",
            "12         0  pm on a saturday night and im in my room feel ...\n",
            "13         0  dollar invest in our mission on xyz thank you ...\n",
            "14         0  year old boy fight for his life and another in...\n",
            "15         0  year ago today we make one of the best choice ...\n",
            "16         0  day until summer xyz on the bright side we onl...\n",
            "17         0  was when in cold blood scar us bruno sammartin...\n",
            "18         0  million project bring journalist coder and hum...\n",
            "19         0  am and i can already tell today will be a grea...\n",
            "20         0  xyz xyz first reaction to the pant i yell what...\n",
            "21         0  d family we want to see all of the fan art you...\n",
            "22         0  d give bore performance it is always the same ...\n",
            "23         0  xyz i have never laugh so much in an interview...\n",
            "24         0             xyz ive never laugh so much in my life\n",
            "25         0  d the king of never promote yet keep their win...\n",
            "26         0  good grade social life excel in your sport sle...\n",
            "27         0  sleep until xyz at dublin own convention centr...\n",
            "28         0  st i wana thank everyone for the call text mes...\n",
            "29         0  st world society help create terrorist b c the...\n",
            "...      ...                                                ...\n",
            "36393      0  xyz xyz enough punishment in itself and no one...\n",
            "36394      0  xyz i do not understand the new youtube rule k...\n",
            "36395      0  xyz if your platform dose not tolerate free sp...\n",
            "36396      0  xyz i would be fuck homeless if i had quit my ...\n",
            "36397      0  xyz just finish watch ungrip highly recommend ...\n",
            "36398      0  xyz xyz i fucc wit u for hit that shoot at lik...\n",
            "36399      0  xyz thank for ruin the best video stream mediu...\n",
            "36400      0  xyz that or you will realize how stupid a deci...\n",
            "36401      0  xyz there own free porn you can watch it for f...\n",
            "36402      0  you have change no i think the proper term is ...\n",
            "36403      0  you have got higher grade than mine not becaus...\n",
            "36404      0  you have got roughly second after the cashier ...\n",
            "36405      0  you wanna be tie down always have to answer to...\n",
            "36406      0  you wanna see a perfect relationship watch a m...\n",
            "36407      0  you wear that shirt a lot yes because i own it...\n",
            "36408      0  you be my cup of tea but i drink champagne now...\n",
            "36409      0                 you wish your be the topic xyz xyz\n",
            "36410      0  xyz i would have to find someone who own good ...\n",
            "36411      0  xyz well the parent was concern like me but ba...\n",
            "36412      0             yup do not i just feel awesome xyz xyz\n",
            "36413      0  yup i am a jumper if i want it im goin for it ...\n",
            "36414      0  xyz tpab has no reply value at all his last al...\n",
            "36415      0  xyz sometimes i wish you would babysit my vagi...\n",
            "36416      0  xyz xyz at all if somebody say that he is an x...\n",
            "36417      0  xyz go and translate the tweet i tweet just be...\n",
            "36418      0  xyz i could see the makeup artist give u an as...\n",
            "36419      0               xyz slvr that own great name xyz xyz\n",
            "36420      0  xyz xyz he is the fag we need but not the fag ...\n",
            "36421      0  zuma sound like kanye west right now try to ex...\n",
            "36422      0  xyz xyz so true student stick around and have ...\n",
            "\n",
            "[36423 rows x 2 columns]\n",
            "0        xyz i hope youre lurk rn i want to listen to h...\n",
            "1        really teach me a valuable lesson i am never g...\n",
            "2        xyz never had a voice to protest so you fed me...\n",
            "3            xyz rest in peace love to you and your family\n",
            "4               day until christmas xyz soon xyz ready yet\n",
            "5        xyz xyz yay can not wait to be reunite with yo...\n",
            "6        word short of the word requirement but i do no...\n",
            "7        xyz it was nice hang out this afternoon i had ...\n",
            "8                k walk this morning we did an awesome job\n",
            "9                                        minute to eat xyz\n",
            "10       time one direction had the perfect love song f...\n",
            "11       xyz yet again if you list them on twitter you ...\n",
            "12               xyz wow sweet butt for all of them i love\n",
            "13                             o clock cant come no faster\n",
            "14       pm on a saturday night and im in my room feel ...\n",
            "15                                 can not come any faster\n",
            "16       dollar invest in our mission on xyz thank you ...\n",
            "17       year old boy fight for his life and another in...\n",
            "18       year ago today we make one of the best choice ...\n",
            "19       day until summer xyz on the bright side we onl...\n",
            "20       was when in cold blood scar us bruno sammartin...\n",
            "21       million project bring journalist coder and hum...\n",
            "22       am and i can already tell today will be a grea...\n",
            "23       xyz xyz first reaction to the pant i yell what...\n",
            "24       d family we want to see all of the fan art you...\n",
            "25       d give bore performance it is always the same ...\n",
            "26       xyz i have never laugh so much in an interview...\n",
            "27                  xyz ive never laugh so much in my life\n",
            "28       d the king of never promote yet keep their win...\n",
            "29       good grade social life excel in your sport sle...\n",
            "                               ...                        \n",
            "39750    xyz i do not understand the new youtube rule k...\n",
            "39751    xyz if your platform dose not tolerate free sp...\n",
            "39752    xyz i would be fuck homeless if i had quit my ...\n",
            "39753    xyz just finish watch ungrip highly recommend ...\n",
            "39754    xyz xyz i fucc wit u for hit that shoot at lik...\n",
            "39755    xyz thank for ruin the best video stream mediu...\n",
            "39756    xyz that or you will realize how stupid a deci...\n",
            "39757    xyz there own free porn you can watch it for f...\n",
            "39758    you have change no i think the proper term is ...\n",
            "39759    you have got higher grade than mine not becaus...\n",
            "39760    you have got roughly second after the cashier ...\n",
            "39761    you wanna be tie down always have to answer to...\n",
            "39762    you wanna see a perfect relationship watch a m...\n",
            "39763    you wear that shirt a lot yes because i own it...\n",
            "39764    you be my cup of tea but i drink champagne now...\n",
            "39765                   you wish your be the topic xyz xyz\n",
            "39766                      yum i love toilet water xyz xyz\n",
            "39767    xyz i would have to find someone who own good ...\n",
            "39768    xyz well the parent was concern like me but ba...\n",
            "39769               yup do not i just feel awesome xyz xyz\n",
            "39770    yup i am a jumper if i want it im goin for it ...\n",
            "39771    xyz tpab has no reply value at all his last al...\n",
            "39772    xyz sometimes i wish you would babysit my vagi...\n",
            "39773    xyz xyz at all if somebody say that he is an x...\n",
            "39774    xyz go and translate the tweet i tweet just be...\n",
            "39775    xyz i could see the makeup artist give u an as...\n",
            "39776                 xyz slvr that own great name xyz xyz\n",
            "39777    xyz xyz he is the fag we need but not the fag ...\n",
            "39778    zuma sound like kanye west right now try to ex...\n",
            "39779    xyz xyz so true student stick around and have ...\n",
            "Name: text, Length: 39780, dtype: object\n",
            "/////////\n",
            "0        xyz i hope youre lurk rn i want to listen to h...\n",
            "1        really teach me a valuable lesson i am never g...\n",
            "2        xyz never had a voice to protest so you fed me...\n",
            "3            xyz rest in peace love to you and your family\n",
            "4               day until christmas xyz soon xyz ready yet\n",
            "5        xyz xyz yay can not wait to be reunite with yo...\n",
            "6        word short of the word requirement but i do no...\n",
            "7        xyz it was nice hang out this afternoon i had ...\n",
            "8                k walk this morning we did an awesome job\n",
            "9        time one direction had the perfect love song f...\n",
            "10       xyz yet again if you list them on twitter you ...\n",
            "11               xyz wow sweet butt for all of them i love\n",
            "12       pm on a saturday night and im in my room feel ...\n",
            "13       dollar invest in our mission on xyz thank you ...\n",
            "14       year old boy fight for his life and another in...\n",
            "15       year ago today we make one of the best choice ...\n",
            "16       day until summer xyz on the bright side we onl...\n",
            "17       was when in cold blood scar us bruno sammartin...\n",
            "18       million project bring journalist coder and hum...\n",
            "19       am and i can already tell today will be a grea...\n",
            "20       xyz xyz first reaction to the pant i yell what...\n",
            "21       d family we want to see all of the fan art you...\n",
            "22       d give bore performance it is always the same ...\n",
            "23       xyz i have never laugh so much in an interview...\n",
            "24                  xyz ive never laugh so much in my life\n",
            "25       d the king of never promote yet keep their win...\n",
            "26       good grade social life excel in your sport sle...\n",
            "27       sleep until xyz at dublin own convention centr...\n",
            "28       st i wana thank everyone for the call text mes...\n",
            "29       st world society help create terrorist b c the...\n",
            "                               ...                        \n",
            "36393    xyz xyz enough punishment in itself and no one...\n",
            "36394    xyz i do not understand the new youtube rule k...\n",
            "36395    xyz if your platform dose not tolerate free sp...\n",
            "36396    xyz i would be fuck homeless if i had quit my ...\n",
            "36397    xyz just finish watch ungrip highly recommend ...\n",
            "36398    xyz xyz i fucc wit u for hit that shoot at lik...\n",
            "36399    xyz thank for ruin the best video stream mediu...\n",
            "36400    xyz that or you will realize how stupid a deci...\n",
            "36401    xyz there own free porn you can watch it for f...\n",
            "36402    you have change no i think the proper term is ...\n",
            "36403    you have got higher grade than mine not becaus...\n",
            "36404    you have got roughly second after the cashier ...\n",
            "36405    you wanna be tie down always have to answer to...\n",
            "36406    you wanna see a perfect relationship watch a m...\n",
            "36407    you wear that shirt a lot yes because i own it...\n",
            "36408    you be my cup of tea but i drink champagne now...\n",
            "36409                   you wish your be the topic xyz xyz\n",
            "36410    xyz i would have to find someone who own good ...\n",
            "36411    xyz well the parent was concern like me but ba...\n",
            "36412               yup do not i just feel awesome xyz xyz\n",
            "36413    yup i am a jumper if i want it im goin for it ...\n",
            "36414    xyz tpab has no reply value at all his last al...\n",
            "36415    xyz sometimes i wish you would babysit my vagi...\n",
            "36416    xyz xyz at all if somebody say that he is an x...\n",
            "36417    xyz go and translate the tweet i tweet just be...\n",
            "36418    xyz i could see the makeup artist give u an as...\n",
            "36419                 xyz slvr that own great name xyz xyz\n",
            "36420    xyz xyz he is the fag we need but not the fag ...\n",
            "36421    zuma sound like kanye west right now try to ex...\n",
            "36422    xyz xyz so true student stick around and have ...\n",
            "Name: text, Length: 36423, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRBQxTrqHgmd"
      },
      "source": [
        "Creating a vocabulary of important words from the twitter dataset which are used later for analysis.Pad-Sequencing is done to convert into array of numbers,as computation will be done on numbers. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O7Vd1xRyWrr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f05d9b4a-3b3a-4d89-c557-dcdb58f76dc1"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "def prepare(q):\n",
        "        new_q = []\n",
        "        unknown = True\n",
        "        for w in q.split()[::-1]:\n",
        "            if w in top_words:\n",
        "                new_q = [w] + new_q\n",
        "                unknown = True\n",
        "            elif w not in STOP_WORDS:\n",
        "                if unknown:\n",
        "                    new_q = [\"XYZ\"] + new_q      ### replacing all names with \"XYZ\" as names are not of utter importance\n",
        "                    unknown = False\n",
        "            else:\n",
        "                unknown = True\n",
        "            if len(new_q) == MAX_SEQUENCE_LENGTH:\n",
        "                break\n",
        "        new_q = \" \".join(new_q)\n",
        "        return new_q\n",
        "      \n",
        "print(\"Creating the vocabulary of words occurred more than\", MIN_WORD_OCCURRENCE)\n",
        "all_text = pd.Series(text2.tolist()).unique()\n",
        "vectorizer = CountVectorizer(lowercase=False, token_pattern=\"\\S+\", min_df=MIN_WORD_OCCURRENCE)\n",
        "vectorizer.fit(all_text)\n",
        "top_words = set(vectorizer.vocabulary_.keys())\n",
        "top_words.add(REPLACE_WORD)\n",
        "\n",
        "irrelevant_names = np.array([\"\"] * len(text2), dtype=object)\n",
        "for i, text in enumerate(text2):\n",
        "  irrelevant_names[i] = prepare(text)\n",
        "\n",
        "tokenizer = Tokenizer(filters=\"\")\n",
        "tokenizer.fit_on_texts(np.array(irrelevant_names))\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "text_data = pad_sequences(tokenizer.texts_to_sequences(irrelevant_names), maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "\n",
        "print(word_index)\n",
        "\n",
        "\n",
        "print(word_index)\n",
        "print(len(word_index))\n",
        "print(top_words)\n",
        "print(len(top_words))\n",
        "print(text_data)\n",
        "vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating the vocabulary of words occurred more than 50\n",
            "{'xyz': 1, 'i': 2, 'to': 3, 'the': 4, 'you': 5, 'a': 6, 'and': 7, 'is': 8, 'not': 9, 'my': 10, 'it': 11, 'of': 12, 'in': 13, 'be': 14, 'for': 15, 'do': 16, 'that': 17, 'me': 18, 'have': 19, 'so': 20, 'love': 21, 'are': 22, 'am': 23, 'on': 24, 'your': 25, 'this': 26, 'like': 27, 'when': 28, 'just': 29, 'with': 30, 'can': 31, 'get': 32, 'but': 33, 'at': 34, 'go': 35, 'own': 36, 'all': 37, 'up': 38, 'day': 39, 'was': 40, 'if': 41, 'will': 42, 'they': 43, 'people': 44, 'we': 45, 'what': 46, 'make': 47, 'out': 48, 'about': 49, 'know': 50, 'how': 51, 'one': 52, 'no': 53, 'time': 54, 'good': 55, 'u': 56, 'say': 57, 'who': 58, 'great': 59, 'think': 60, 'as': 61, 'an': 62, 'want': 63, 'really': 64, 'would': 65, 'retweet': 66, 'look': 67, 'he': 68, 'work': 69, 'see': 70, 'from': 71, 'need': 72, 'life': 73, 'thank': 74, 'much': 75, 'now': 76, 'feel': 77, 'wait': 78, 'or': 79, 'there': 80, 'she': 81, 'more': 82, 'thing': 83, 'back': 84, 'because': 85, 'school': 86, 'come': 87, 'take': 88, 'by': 89, 'fuck': 90, 'sleep': 91, 'today': 92, 'why': 93, 'did': 94, 'her': 95, 'got': 96, 'then': 97, 'them': 98, 'too': 99, 'off': 100, 'hate': 101, 'happy': 102, 'never': 103, 'shit': 104, 'friend': 105, 'lol': 106, 'only': 107, 'someone': 108, 'has': 109, 'right': 110, 'best': 111, 'had': 112, 'night': 113, 'some': 114, 'year': 115, 'start': 116, 'even': 117, 'tell': 118, 'class': 119, 'always': 120, 'give': 121, 'girl': 122, 'their': 123, 'than': 124, 'still': 125, 'home': 126, 'ever': 127, 'try': 128, 'could': 129, 'our': 130, 'way': 131, 'wake': 132, 'should': 133, 'his': 134, 'talk': 135, 'watch': 136, 'fun': 137, 'nothing': 138, 'its': 139, 'last': 140, 'first': 141, 'hour': 142, 'im': 143, 'here': 144, 'after': 145, 'over': 146, 'better': 147, 'new': 148, 'guy': 149, 'morning': 150, 'doe': 151, 'call': 152, 'well': 153, 'tomorrow': 154, 'every': 155, 'let': 156, 'week': 157, 'find': 158, 'down': 159, 'keep': 160, 'gonna': 161, 'before': 162, 'him': 163, 'yay': 164, 'something': 165, 'use': 166, 'play': 167, 'person': 168, 'other': 169, 'everyone': 170, 'oh': 171, 'please': 172, 'lose': 173, 'game': 174, 'hope': 175, 'kid': 176, 'man': 177, 'phone': 178, 'us': 179, 'wish': 180, 'leave': 181, 'miss': 182, 'long': 183, 'mean': 184, 'nice': 185, 'next': 186, 'stop': 187, 'into': 188, 'ask': 189, 'where': 190, 'bed': 191, 'these': 192, 'woman': 193, 'show': 194, 'tonight': 195, 'again': 196, 'world': 197, 'literally': 198, 'excite': 199, 'bitch': 200, 'care': 201, 'hear': 202, 'put': 203, 'rain': 204, 'birthday': 205, 'homework': 206, 'pretty': 207, 'mom': 208, 'everything': 209, 'break': 210, 'drink': 211, 'face': 212, 'live': 213, 'hair': 214, 'bad': 215, 'happen': 216, 'weekend': 217, 'yeah': 218, 'most': 219, 'while': 220, 'any': 221, 'god': 222, 'real': 223, 'awesome': 224, 'same': 225, 'lot': 226, 'very': 227, 'actually': 228, 'end': 229, 'sure': 230, 'dont': 231, 'laugh': 232, 'eat': 233, 'test': 234, 'money': 235, 'those': 236, 'little': 237, 'lie': 238, 'house': 239, 'food': 240, 'help': 241, 'without': 242, 'tweet': 243, 'such': 244, 'around': 245, 'fall': 246, 'song': 247, 'cool': 248, 'another': 249, 'wow': 250, 'two': 251, 'stay': 252, 'glad': 253, 'enjoy': 254, 'boy': 255, 'room': 256, 'old': 257, 'wanna': 258, 'full': 259, 'text': 260, 'ass': 261, 'run': 262, 'until': 263, 'already': 264, 'anything': 265, 'cause': 266, 'many': 267, 'forward': 268, 'follow': 269, 'turn': 270, 'parent': 271, 'myself': 272, 'hard': 273, 'read': 274, 'buy': 275, 'twitter': 276, 'sick': 277, 'beautiful': 278, 'job': 279, 'remember': 280, 'listen': 281, 'meet': 282, 'amaze': 283, 'free': 284, 'family': 285, 'change': 286, 'walk': 287, 'yes': 288, 'half': 289, 'minute': 290, 'dollar': 291, 'forget': 292, 'cute': 293, 'moment': 294, 'kill': 295, 'bring': 296, 'favorite': 297, 'though': 298, 'cant': 299, 'drive': 300, 'big': 301, 'bc': 302, 'early': 303, 'enough': 304, 'percent': 305, 'word': 306, 'away': 307, 'reason': 308, 'pay': 309, 'friday': 310, 'car': 311, 'second': 312, 'music': 313, 'sit': 314, 'name': 315, 'win': 316, 'funny': 317, 'video': 318, 'baby': 319, 'also': 320, 'suck': 321, 'die': 322, 'child': 323, 'cry': 324, 'idea': 325, 'through': 326, 'hit': 327, 'wear': 328, 'teacher': 329, 'learn': 330, 'hurt': 331, 'realize': 332, 'anyone': 333, 'post': 334, 'wrong': 335, 'relationship': 336, 'mind': 337, 'whole': 338, 'send': 339, 'head': 340, 'high': 341, 'swear': 342, 'hot': 343, 'okay': 344, 'sorry': 345, 'summer': 346, 'since': 347, 'else': 348, 'bless': 349, 'fight': 350, 'spend': 351, 'awkward': 352, 'heart': 353, 'probably': 354, 'yet': 355, 'finish': 356, 'move': 357, 'guess': 358, 'tire': 359, 'sometimes': 360, 'check': 361, 'ok': 362, 'late': 363, 'sad': 364, 'super': 365, 'together': 366, 'fan': 367, 'plus': 368, 'hey': 369, 'dream': 370, 'sound': 371, 'shower': 372, 'y': 373, 'cold': 374, 'maybe': 375, 'write': 376, 'part': 377, 'place': 378, 'w': 379, 'fire': 380, 'stupid': 381, 'eye': 382, 'problem': 383, 'picture': 384, 'understand': 385, 'which': 386, 'yourself': 387, 'n': 388, 'side': 389, 'stress': 390, 'movie': 391, 'catch': 392, 'once': 393, 'might': 394, 'hop': 395, 'dress': 396, 'interest': 397, 'perfect': 398, 'd': 399, 'worst': 400, 'must': 401, 'college': 402, 'smile': 403, 'month': 404, 'true': 405, 'omg': 406, 'fact': 407, 'believe': 408, 'dumb': 409, 'facebook': 410, 'yell': 411, 'ai': 412, 'credit': 413, 'point': 414, 'book': 415, 'date': 416, 'team': 417, 'question': 418, 'sister': 419, 'open': 420, 'smoke': 421, 'damn': 422, 'wonder': 423, 'become': 424, 'study': 425, 'boyfriend': 426, 'crazy': 427, 'plan': 428, 'haha': 429, 'dark': 430, 'dad': 431, 'dog': 432, 'soon': 433, 'r': 434, 'football': 435, 'mood': 436, 'ready': 437, 'doctor': 438, 'hell': 439, 'gotta': 440, 'hand': 441, 'reply': 442, 'piss': 443, 'almost': 444, 'proud': 445, 'may': 446, 'cover': 447, 'sing': 448, 'rest': 449, 'appreciate': 450, 'story': 451, 'stand': 452, 'finally': 453, 'clean': 454, 'black': 455, 'k': 456, 'seem': 457, 'act': 458, 'alarm': 459, 'mad': 460, 'each': 461, 'stick': 462, 'asleep': 463, 'both': 464, 'fail': 465, 'men': 466, 'vote': 467, 'smell': 468, 'bus': 469, 'ago': 470, 'dead': 471, 'set': 472, 'alone': 473, 'least': 474, 'ya': 475, 'answer': 476, 'kind': 477, 'ignore': 478, 'nobody': 479, 'close': 480, 'tv': 481, 'instead': 482, 'matter': 483, 'few': 484, 'nigga': 485, 'equal': 486, 'lmao': 487, 'trump': 488, 'o': 489, 'season': 490, 'between': 491, 'single': 492, 'water': 493, 's': 494, 'card': 495, 'body': 496, 'exam': 497, 'album': 498, 'x': 499, 'light': 500, 'able': 501, 'leg': 502, 'shitty': 503, 'annoy': 504, 'party': 505, 'under': 506, 'conversation': 507, 'christmas': 508, 'till': 509, 'decide': 510, 'hold': 511, 'sweet': 512, 'lay': 513, 'type': 514, 'white': 515, 'th': 516, 'anymore': 517, 'during': 518, 'top': 519, 'joke': 520, 'build': 521, 'bore': 522, 'busy': 523, 'cat': 524, 'fantastic': 525, 'throw': 526, 'outside': 527, 'monday': 528, 'either': 529, 'pic': 530, 'ugly': 531, 'worse': 532, 'girlfriend': 533, 'stuff': 534, 'dude': 535, 'middle': 536, 'weird': 537, 'worry': 538, 'tho': 539, 'grow': 540, 'sex': 541, 'b': 542, 'far': 543, 'saw': 544, 'country': 545, 'photo': 546, 'save': 547, 'wet': 548, 'roll': 549, 'lunch': 550, 'hoe': 551, 'share': 552, 'later': 553, 'freak': 554, 'dance': 555, 'cancel': 556, 'deserve': 557, 'wtf': 558, 'scar': 559, 'door': 560, 'awake': 561, 'business': 562, 'joy': 563, 'argument': 564, 'support': 565, 'trust': 566, 'front': 567, 'student': 568, 'speak': 569, 'shame': 570, 'news': 571, 'butt': 572, 'cough': 573, 'pm': 574, 'le': 575, 'weather': 576, 'nap': 577, 'shoot': 578, 'lab': 579, 'message': 580, 'history': 581, 'absolutely': 582, 'pick': 583, 'yesterday': 584, 'math': 585, 'handle': 586, 'drop': 587, 'suppose': 588, 'wonderful': 589, 'seriously': 590, 'internet': 591, 'hug': 592, 'dear': 593, 'sign': 594, 'idk': 595, 'lovely': 596, 't': 597, 'agree': 598, 'state': 599, 'surprise': 600, 'honestly': 601, 'shock': 602, 'human': 603, 'everyday': 604, 'shave': 605, 'thanksgiving': 606, 'past': 607, 'pack': 608, 'saturday': 609, 'prayer': 610, 'judge': 611, 'expect': 612, 'kinda': 613, 'medium': 614, 'arm': 615, 'hilarious': 616, 'deal': 617, 'charge': 618, 'chance': 619, 'short': 620, 'release': 621, 'quite': 622, 'coffee': 623, 'order': 624, 'straight': 625, 'burn': 626, 'holiday': 627, 'different': 628, 'star': 629, 'blood': 630, 'via': 631, 'shout': 632, 'shift': 633, 'hang': 634, 'bear': 635, 'comment': 636, 'sunday': 637, 'teach': 638, 'social': 639, 'beat': 640, 'brother': 641, 'successful': 642, 'marry': 643, 'pas': 644, 'paper': 645, 'future': 646, 'behind': 647, 'entire': 648, 'voice': 649, 'shut': 650, 'worth': 651, 'visit': 652, 'safe': 653, 'america': 654, 'american': 655, 'add': 656, 'sarcastic': 657, 'pull': 658, 'bit': 659, 'three': 660, 'number': 661, 'forever': 662, 'lady': 663, 'gas': 664, 'dinner': 665, 'hillary': 666, 'hi': 667, 'important': 668, 'fake': 669, 'ride': 670, 'hide': 671, 'download': 672, 'blast': 673, 'others': 674, 'store': 675, 'cut': 676, 'honest': 677, 'due': 678, 'line': 679, 'thats': 680, 'argue': 681, 'piece': 682, 'couple': 683, 'waste': 684, 'fridge': 685, 'ex': 686, 'case': 687, 'pretend': 688, 'hungry': 689, 'self': 690, 'peace': 691, 'direction': 692, 'physic': 693, 'respect': 694, 'fix': 695, 'everybody': 696, 'imagine': 697, 'attempt': 698, 'luck': 699, 'ruin': 700, 'mouth': 701, 'inside': 702, 'rather': 703, 'wash': 704, 'limit': 705, 'min': 706, 'smart': 707, 'level': 708, 'control': 709, 'account': 710, 'clothe': 711, 'list': 712, 'fine': 713, 'group': 714, 'goal': 715, 'grade': 716, 'secret': 717, 'treat': 718, 'cuz': 719, 'mum': 720, 'af': 721, 'unless': 722, 'against': 723, 'wife': 724, 'm': 725, 'kiss': 726, 'bestfriend': 727, 'exactly': 728, 'upset': 729, 'invest': 730, 'million': 731, 'pant': 732, 'p': 733, 'negative': 734, 'dick': 735, 'easy': 736, 'freshly': 737, 'crush': 738, 'upstairs': 739, 'choice': 740, 'extra': 741, 'course': 742, 'mine': 743, 'totally': 744, 'subtle': 745, 'whenever': 746, 'st': 747, 'fast': 748, 'allow': 749, 'creepy': 750, 'sneeze': 751, 'mess': 752, 'elephant': 753, 'notice': 754, 'truth': 755, 'young': 756, 'semester': 757, 'hive': 758, 'grass': 759, 'pizza': 760, 'gym': 761, 'bank': 762, 'example': 763, 'loud': 764, 'bullshit': 765, 'yo': 766, 'sense': 767, 'age': 768, 'strong': 769, 'babysit': 770, 'update': 771, 'fat': 772, 'wide': 773, 'player': 774, 'obama': 775, 'absolute': 776, 'gorgeous': 777, 'career': 778, 'special': 779, 'bro': 780, 'definitely': 781, 'insult': 782, 'jerk': 783, 'justin': 784, 'episode': 785, 'virginity': 786, 'issue': 787, 'brain': 788, 'memory': 789, 'choose': 790, 'track': 791, 'train': 792, 'tea': 793, 'join': 794, 'delete': 795, 'met': 796, 'shop': 797, 'f': 798, 'idiot': 799, 'happiness': 800, 'power': 801, 'bag': 802, 'pour': 803, 'drug': 804, 'police': 805, 'president': 806, 'clock': 807, 'c': 808, 'create': 809, 'arrest': 810, 'kick': 811, 'bet': 812, 'finger': 813, 'explain': 814, 'whatever': 815, 'lil': 816, 'english': 817, 'email': 818, 'lazy': 819, 'race': 820, 'sun': 821, 'begin': 822, 'ha': 823, 'scream': 824, 'neighbor': 825, 'doubt': 826, 'babysitting': 827, 'normal': 828, 'google': 829, 'trip': 830, 'angry': 831, 'hospital': 832, 'promise': 833, 'huge': 834, 'amuse': 835, 'pray': 836, 'serve': 837, 'attack': 838, 'blow': 839, 'assignment': 840, 'public': 841, 'ball': 842, 'attention': 843, 'note': 844, 'count': 845, 'attractive': 846, 'office': 847, 'chill': 848, 'except': 849, 'pain': 850, 'block': 851, 'puke': 852, 'character': 853, 'follower': 854, 'draw': 855, 'death': 856, 'foot': 857, 'son': 858, 'em': 859, 'five': 860, 'truly': 861, 'gay': 862, 'somebody': 863, 'bright': 864, 'tattoo': 865, 'nightmare': 866, 'touch': 867, 'iphone': 868, 'especially': 869, 'success': 870, 'adult': 871, 'poor': 872, 'letter': 873, 'mistake': 874, 'service': 875, 'mother': 876, 'random': 877, 'road': 878, 'hahaha': 879, 'lord': 880, 'city': 881, 'decision': 882, 'tax': 883, 'final': 884, 'amazin': 885, 'algebra': 886, 'floor': 887, 'schedule': 888, 'ticket': 889, 'report': 890, 'bathroom': 891, 'goodnight': 892, 'reach': 893, 'traffic': 894, 'taste': 895, 'mr': 896, 'gift': 897, 'rn': 898, 'convention': 899, 'practice': 900, 'continue': 901, 'hello': 902, 'match': 903, 'usually': 904, 'beer': 905, 'completely': 906, 'disappointment': 907, 'smh': 908, 'impressive': 909, 'sweat': 910, 'war': 911, 'page': 912, 'four': 913, 'sell': 914, 'license': 915, 'rock': 916, 'welcome': 917, 'instagram': 918, 'step': 919, 'return': 920, 'pls': 921, 'club': 922, 'lucky': 923, 'greener': 924, 'shirt': 925, 'band': 926, 'tuesday': 927, 'winter': 928, 'tear': 929, 'cook': 930, 'computer': 931, 'bout': 932, 'health': 933, 'chemistry': 934, 'art': 935, 'interview': 936, 'wall': 937, 'degree': 938, 'frustration': 939, 'experience': 940, 'hockey': 941, 'view': 942, 'possible': 943, 'drown': 944, 'weight': 945, 'low': 946, 'longer': 947, 'apple': 948, 'lead': 949, 'determine': 950, 'online': 951, 'youtube': 952, 'street': 953, 'small': 954, 'closer': 955, 'raise': 956, 'android': 957, 'park': 958, 'bunch': 959, 'sore': 960, 'bill': 961, 'key': 962, 'fill': 963, 'harry': 964, 'sport': 965, 'app': 966, 'apparently': 967, 'female': 968, 'bye': 969, 'terrible': 970, 'loose': 971, 'record': 972, 'snow': 973, 'snapchat': 974, 'faster': 975, 'soul': 976, 'consider': 977, 'rich': 978, 'serious': 979, 'jump': 980, 'felt': 981, 'difference': 982, 'attract': 983, 'hehehe': 984, 'prepare': 985, 'carry': 986, 'event': 987, 'button': 988, 'law': 989, 'government': 990, 'mention': 991, 'dry': 992, 'ill': 993, 'fag': 994, 'xx': 995, 'roast': 996, 'nd': 997, 'won': 998, 'figure': 999, 'complain': 1000, 'swim': 1001, 'anger': 1002, 'push': 1003, 'ugh': 1004, 'sale': 1005, 'rule': 1006, 'thursday': 1007, 'window': 1008, 'easily': 1009, 'project': 1010, 'personal': 1011, 'makeup': 1012, 'company': 1013, 'film': 1014, 'concert': 1015, 'highschool': 1016, 'remind': 1017, 'basically': 1018, 'pass': 1019, 'quick': 1020, 'didnt': 1021, 'league': 1022, 'pop': 1023, 'ring': 1024, 'afraid': 1025, 'cigarette': 1026, 'campus': 1027, 'warm': 1028, 'ive': 1029, 'period': 1030, 'biggest': 1031, 'space': 1032, 'costa': 1033, 'fly': 1034, 'ahead': 1035, 'throat': 1036, 'slow': 1037, 'intimidate': 1038, 'air': 1039, 'babe': 1040, 'situation': 1041, 'bottle': 1042, 'deep': 1043, 'opinion': 1044, 'cycle': 1045, 'red': 1046, 'along': 1047, 'often': 1048, 'alive': 1049, 'prove': 1050}\n",
            "{'xyz': 1, 'i': 2, 'to': 3, 'the': 4, 'you': 5, 'a': 6, 'and': 7, 'is': 8, 'not': 9, 'my': 10, 'it': 11, 'of': 12, 'in': 13, 'be': 14, 'for': 15, 'do': 16, 'that': 17, 'me': 18, 'have': 19, 'so': 20, 'love': 21, 'are': 22, 'am': 23, 'on': 24, 'your': 25, 'this': 26, 'like': 27, 'when': 28, 'just': 29, 'with': 30, 'can': 31, 'get': 32, 'but': 33, 'at': 34, 'go': 35, 'own': 36, 'all': 37, 'up': 38, 'day': 39, 'was': 40, 'if': 41, 'will': 42, 'they': 43, 'people': 44, 'we': 45, 'what': 46, 'make': 47, 'out': 48, 'about': 49, 'know': 50, 'how': 51, 'one': 52, 'no': 53, 'time': 54, 'good': 55, 'u': 56, 'say': 57, 'who': 58, 'great': 59, 'think': 60, 'as': 61, 'an': 62, 'want': 63, 'really': 64, 'would': 65, 'retweet': 66, 'look': 67, 'he': 68, 'work': 69, 'see': 70, 'from': 71, 'need': 72, 'life': 73, 'thank': 74, 'much': 75, 'now': 76, 'feel': 77, 'wait': 78, 'or': 79, 'there': 80, 'she': 81, 'more': 82, 'thing': 83, 'back': 84, 'because': 85, 'school': 86, 'come': 87, 'take': 88, 'by': 89, 'fuck': 90, 'sleep': 91, 'today': 92, 'why': 93, 'did': 94, 'her': 95, 'got': 96, 'then': 97, 'them': 98, 'too': 99, 'off': 100, 'hate': 101, 'happy': 102, 'never': 103, 'shit': 104, 'friend': 105, 'lol': 106, 'only': 107, 'someone': 108, 'has': 109, 'right': 110, 'best': 111, 'had': 112, 'night': 113, 'some': 114, 'year': 115, 'start': 116, 'even': 117, 'tell': 118, 'class': 119, 'always': 120, 'give': 121, 'girl': 122, 'their': 123, 'than': 124, 'still': 125, 'home': 126, 'ever': 127, 'try': 128, 'could': 129, 'our': 130, 'way': 131, 'wake': 132, 'should': 133, 'his': 134, 'talk': 135, 'watch': 136, 'fun': 137, 'nothing': 138, 'its': 139, 'last': 140, 'first': 141, 'hour': 142, 'im': 143, 'here': 144, 'after': 145, 'over': 146, 'better': 147, 'new': 148, 'guy': 149, 'morning': 150, 'doe': 151, 'call': 152, 'well': 153, 'tomorrow': 154, 'every': 155, 'let': 156, 'week': 157, 'find': 158, 'down': 159, 'keep': 160, 'gonna': 161, 'before': 162, 'him': 163, 'yay': 164, 'something': 165, 'use': 166, 'play': 167, 'person': 168, 'other': 169, 'everyone': 170, 'oh': 171, 'please': 172, 'lose': 173, 'game': 174, 'hope': 175, 'kid': 176, 'man': 177, 'phone': 178, 'us': 179, 'wish': 180, 'leave': 181, 'miss': 182, 'long': 183, 'mean': 184, 'nice': 185, 'next': 186, 'stop': 187, 'into': 188, 'ask': 189, 'where': 190, 'bed': 191, 'these': 192, 'woman': 193, 'show': 194, 'tonight': 195, 'again': 196, 'world': 197, 'literally': 198, 'excite': 199, 'bitch': 200, 'care': 201, 'hear': 202, 'put': 203, 'rain': 204, 'birthday': 205, 'homework': 206, 'pretty': 207, 'mom': 208, 'everything': 209, 'break': 210, 'drink': 211, 'face': 212, 'live': 213, 'hair': 214, 'bad': 215, 'happen': 216, 'weekend': 217, 'yeah': 218, 'most': 219, 'while': 220, 'any': 221, 'god': 222, 'real': 223, 'awesome': 224, 'same': 225, 'lot': 226, 'very': 227, 'actually': 228, 'end': 229, 'sure': 230, 'dont': 231, 'laugh': 232, 'eat': 233, 'test': 234, 'money': 235, 'those': 236, 'little': 237, 'lie': 238, 'house': 239, 'food': 240, 'help': 241, 'without': 242, 'tweet': 243, 'such': 244, 'around': 245, 'fall': 246, 'song': 247, 'cool': 248, 'another': 249, 'wow': 250, 'two': 251, 'stay': 252, 'glad': 253, 'enjoy': 254, 'boy': 255, 'room': 256, 'old': 257, 'wanna': 258, 'full': 259, 'text': 260, 'ass': 261, 'run': 262, 'until': 263, 'already': 264, 'anything': 265, 'cause': 266, 'many': 267, 'forward': 268, 'follow': 269, 'turn': 270, 'parent': 271, 'myself': 272, 'hard': 273, 'read': 274, 'buy': 275, 'twitter': 276, 'sick': 277, 'beautiful': 278, 'job': 279, 'remember': 280, 'listen': 281, 'meet': 282, 'amaze': 283, 'free': 284, 'family': 285, 'change': 286, 'walk': 287, 'yes': 288, 'half': 289, 'minute': 290, 'dollar': 291, 'forget': 292, 'cute': 293, 'moment': 294, 'kill': 295, 'bring': 296, 'favorite': 297, 'though': 298, 'cant': 299, 'drive': 300, 'big': 301, 'bc': 302, 'early': 303, 'enough': 304, 'percent': 305, 'word': 306, 'away': 307, 'reason': 308, 'pay': 309, 'friday': 310, 'car': 311, 'second': 312, 'music': 313, 'sit': 314, 'name': 315, 'win': 316, 'funny': 317, 'video': 318, 'baby': 319, 'also': 320, 'suck': 321, 'die': 322, 'child': 323, 'cry': 324, 'idea': 325, 'through': 326, 'hit': 327, 'wear': 328, 'teacher': 329, 'learn': 330, 'hurt': 331, 'realize': 332, 'anyone': 333, 'post': 334, 'wrong': 335, 'relationship': 336, 'mind': 337, 'whole': 338, 'send': 339, 'head': 340, 'high': 341, 'swear': 342, 'hot': 343, 'okay': 344, 'sorry': 345, 'summer': 346, 'since': 347, 'else': 348, 'bless': 349, 'fight': 350, 'spend': 351, 'awkward': 352, 'heart': 353, 'probably': 354, 'yet': 355, 'finish': 356, 'move': 357, 'guess': 358, 'tire': 359, 'sometimes': 360, 'check': 361, 'ok': 362, 'late': 363, 'sad': 364, 'super': 365, 'together': 366, 'fan': 367, 'plus': 368, 'hey': 369, 'dream': 370, 'sound': 371, 'shower': 372, 'y': 373, 'cold': 374, 'maybe': 375, 'write': 376, 'part': 377, 'place': 378, 'w': 379, 'fire': 380, 'stupid': 381, 'eye': 382, 'problem': 383, 'picture': 384, 'understand': 385, 'which': 386, 'yourself': 387, 'n': 388, 'side': 389, 'stress': 390, 'movie': 391, 'catch': 392, 'once': 393, 'might': 394, 'hop': 395, 'dress': 396, 'interest': 397, 'perfect': 398, 'd': 399, 'worst': 400, 'must': 401, 'college': 402, 'smile': 403, 'month': 404, 'true': 405, 'omg': 406, 'fact': 407, 'believe': 408, 'dumb': 409, 'facebook': 410, 'yell': 411, 'ai': 412, 'credit': 413, 'point': 414, 'book': 415, 'date': 416, 'team': 417, 'question': 418, 'sister': 419, 'open': 420, 'smoke': 421, 'damn': 422, 'wonder': 423, 'become': 424, 'study': 425, 'boyfriend': 426, 'crazy': 427, 'plan': 428, 'haha': 429, 'dark': 430, 'dad': 431, 'dog': 432, 'soon': 433, 'r': 434, 'football': 435, 'mood': 436, 'ready': 437, 'doctor': 438, 'hell': 439, 'gotta': 440, 'hand': 441, 'reply': 442, 'piss': 443, 'almost': 444, 'proud': 445, 'may': 446, 'cover': 447, 'sing': 448, 'rest': 449, 'appreciate': 450, 'story': 451, 'stand': 452, 'finally': 453, 'clean': 454, 'black': 455, 'k': 456, 'seem': 457, 'act': 458, 'alarm': 459, 'mad': 460, 'each': 461, 'stick': 462, 'asleep': 463, 'both': 464, 'fail': 465, 'men': 466, 'vote': 467, 'smell': 468, 'bus': 469, 'ago': 470, 'dead': 471, 'set': 472, 'alone': 473, 'least': 474, 'ya': 475, 'answer': 476, 'kind': 477, 'ignore': 478, 'nobody': 479, 'close': 480, 'tv': 481, 'instead': 482, 'matter': 483, 'few': 484, 'nigga': 485, 'equal': 486, 'lmao': 487, 'trump': 488, 'o': 489, 'season': 490, 'between': 491, 'single': 492, 'water': 493, 's': 494, 'card': 495, 'body': 496, 'exam': 497, 'album': 498, 'x': 499, 'light': 500, 'able': 501, 'leg': 502, 'shitty': 503, 'annoy': 504, 'party': 505, 'under': 506, 'conversation': 507, 'christmas': 508, 'till': 509, 'decide': 510, 'hold': 511, 'sweet': 512, 'lay': 513, 'type': 514, 'white': 515, 'th': 516, 'anymore': 517, 'during': 518, 'top': 519, 'joke': 520, 'build': 521, 'bore': 522, 'busy': 523, 'cat': 524, 'fantastic': 525, 'throw': 526, 'outside': 527, 'monday': 528, 'either': 529, 'pic': 530, 'ugly': 531, 'worse': 532, 'girlfriend': 533, 'stuff': 534, 'dude': 535, 'middle': 536, 'weird': 537, 'worry': 538, 'tho': 539, 'grow': 540, 'sex': 541, 'b': 542, 'far': 543, 'saw': 544, 'country': 545, 'photo': 546, 'save': 547, 'wet': 548, 'roll': 549, 'lunch': 550, 'hoe': 551, 'share': 552, 'later': 553, 'freak': 554, 'dance': 555, 'cancel': 556, 'deserve': 557, 'wtf': 558, 'scar': 559, 'door': 560, 'awake': 561, 'business': 562, 'joy': 563, 'argument': 564, 'support': 565, 'trust': 566, 'front': 567, 'student': 568, 'speak': 569, 'shame': 570, 'news': 571, 'butt': 572, 'cough': 573, 'pm': 574, 'le': 575, 'weather': 576, 'nap': 577, 'shoot': 578, 'lab': 579, 'message': 580, 'history': 581, 'absolutely': 582, 'pick': 583, 'yesterday': 584, 'math': 585, 'handle': 586, 'drop': 587, 'suppose': 588, 'wonderful': 589, 'seriously': 590, 'internet': 591, 'hug': 592, 'dear': 593, 'sign': 594, 'idk': 595, 'lovely': 596, 't': 597, 'agree': 598, 'state': 599, 'surprise': 600, 'honestly': 601, 'shock': 602, 'human': 603, 'everyday': 604, 'shave': 605, 'thanksgiving': 606, 'past': 607, 'pack': 608, 'saturday': 609, 'prayer': 610, 'judge': 611, 'expect': 612, 'kinda': 613, 'medium': 614, 'arm': 615, 'hilarious': 616, 'deal': 617, 'charge': 618, 'chance': 619, 'short': 620, 'release': 621, 'quite': 622, 'coffee': 623, 'order': 624, 'straight': 625, 'burn': 626, 'holiday': 627, 'different': 628, 'star': 629, 'blood': 630, 'via': 631, 'shout': 632, 'shift': 633, 'hang': 634, 'bear': 635, 'comment': 636, 'sunday': 637, 'teach': 638, 'social': 639, 'beat': 640, 'brother': 641, 'successful': 642, 'marry': 643, 'pas': 644, 'paper': 645, 'future': 646, 'behind': 647, 'entire': 648, 'voice': 649, 'shut': 650, 'worth': 651, 'visit': 652, 'safe': 653, 'america': 654, 'american': 655, 'add': 656, 'sarcastic': 657, 'pull': 658, 'bit': 659, 'three': 660, 'number': 661, 'forever': 662, 'lady': 663, 'gas': 664, 'dinner': 665, 'hillary': 666, 'hi': 667, 'important': 668, 'fake': 669, 'ride': 670, 'hide': 671, 'download': 672, 'blast': 673, 'others': 674, 'store': 675, 'cut': 676, 'honest': 677, 'due': 678, 'line': 679, 'thats': 680, 'argue': 681, 'piece': 682, 'couple': 683, 'waste': 684, 'fridge': 685, 'ex': 686, 'case': 687, 'pretend': 688, 'hungry': 689, 'self': 690, 'peace': 691, 'direction': 692, 'physic': 693, 'respect': 694, 'fix': 695, 'everybody': 696, 'imagine': 697, 'attempt': 698, 'luck': 699, 'ruin': 700, 'mouth': 701, 'inside': 702, 'rather': 703, 'wash': 704, 'limit': 705, 'min': 706, 'smart': 707, 'level': 708, 'control': 709, 'account': 710, 'clothe': 711, 'list': 712, 'fine': 713, 'group': 714, 'goal': 715, 'grade': 716, 'secret': 717, 'treat': 718, 'cuz': 719, 'mum': 720, 'af': 721, 'unless': 722, 'against': 723, 'wife': 724, 'm': 725, 'kiss': 726, 'bestfriend': 727, 'exactly': 728, 'upset': 729, 'invest': 730, 'million': 731, 'pant': 732, 'p': 733, 'negative': 734, 'dick': 735, 'easy': 736, 'freshly': 737, 'crush': 738, 'upstairs': 739, 'choice': 740, 'extra': 741, 'course': 742, 'mine': 743, 'totally': 744, 'subtle': 745, 'whenever': 746, 'st': 747, 'fast': 748, 'allow': 749, 'creepy': 750, 'sneeze': 751, 'mess': 752, 'elephant': 753, 'notice': 754, 'truth': 755, 'young': 756, 'semester': 757, 'hive': 758, 'grass': 759, 'pizza': 760, 'gym': 761, 'bank': 762, 'example': 763, 'loud': 764, 'bullshit': 765, 'yo': 766, 'sense': 767, 'age': 768, 'strong': 769, 'babysit': 770, 'update': 771, 'fat': 772, 'wide': 773, 'player': 774, 'obama': 775, 'absolute': 776, 'gorgeous': 777, 'career': 778, 'special': 779, 'bro': 780, 'definitely': 781, 'insult': 782, 'jerk': 783, 'justin': 784, 'episode': 785, 'virginity': 786, 'issue': 787, 'brain': 788, 'memory': 789, 'choose': 790, 'track': 791, 'train': 792, 'tea': 793, 'join': 794, 'delete': 795, 'met': 796, 'shop': 797, 'f': 798, 'idiot': 799, 'happiness': 800, 'power': 801, 'bag': 802, 'pour': 803, 'drug': 804, 'police': 805, 'president': 806, 'clock': 807, 'c': 808, 'create': 809, 'arrest': 810, 'kick': 811, 'bet': 812, 'finger': 813, 'explain': 814, 'whatever': 815, 'lil': 816, 'english': 817, 'email': 818, 'lazy': 819, 'race': 820, 'sun': 821, 'begin': 822, 'ha': 823, 'scream': 824, 'neighbor': 825, 'doubt': 826, 'babysitting': 827, 'normal': 828, 'google': 829, 'trip': 830, 'angry': 831, 'hospital': 832, 'promise': 833, 'huge': 834, 'amuse': 835, 'pray': 836, 'serve': 837, 'attack': 838, 'blow': 839, 'assignment': 840, 'public': 841, 'ball': 842, 'attention': 843, 'note': 844, 'count': 845, 'attractive': 846, 'office': 847, 'chill': 848, 'except': 849, 'pain': 850, 'block': 851, 'puke': 852, 'character': 853, 'follower': 854, 'draw': 855, 'death': 856, 'foot': 857, 'son': 858, 'em': 859, 'five': 860, 'truly': 861, 'gay': 862, 'somebody': 863, 'bright': 864, 'tattoo': 865, 'nightmare': 866, 'touch': 867, 'iphone': 868, 'especially': 869, 'success': 870, 'adult': 871, 'poor': 872, 'letter': 873, 'mistake': 874, 'service': 875, 'mother': 876, 'random': 877, 'road': 878, 'hahaha': 879, 'lord': 880, 'city': 881, 'decision': 882, 'tax': 883, 'final': 884, 'amazin': 885, 'algebra': 886, 'floor': 887, 'schedule': 888, 'ticket': 889, 'report': 890, 'bathroom': 891, 'goodnight': 892, 'reach': 893, 'traffic': 894, 'taste': 895, 'mr': 896, 'gift': 897, 'rn': 898, 'convention': 899, 'practice': 900, 'continue': 901, 'hello': 902, 'match': 903, 'usually': 904, 'beer': 905, 'completely': 906, 'disappointment': 907, 'smh': 908, 'impressive': 909, 'sweat': 910, 'war': 911, 'page': 912, 'four': 913, 'sell': 914, 'license': 915, 'rock': 916, 'welcome': 917, 'instagram': 918, 'step': 919, 'return': 920, 'pls': 921, 'club': 922, 'lucky': 923, 'greener': 924, 'shirt': 925, 'band': 926, 'tuesday': 927, 'winter': 928, 'tear': 929, 'cook': 930, 'computer': 931, 'bout': 932, 'health': 933, 'chemistry': 934, 'art': 935, 'interview': 936, 'wall': 937, 'degree': 938, 'frustration': 939, 'experience': 940, 'hockey': 941, 'view': 942, 'possible': 943, 'drown': 944, 'weight': 945, 'low': 946, 'longer': 947, 'apple': 948, 'lead': 949, 'determine': 950, 'online': 951, 'youtube': 952, 'street': 953, 'small': 954, 'closer': 955, 'raise': 956, 'android': 957, 'park': 958, 'bunch': 959, 'sore': 960, 'bill': 961, 'key': 962, 'fill': 963, 'harry': 964, 'sport': 965, 'app': 966, 'apparently': 967, 'female': 968, 'bye': 969, 'terrible': 970, 'loose': 971, 'record': 972, 'snow': 973, 'snapchat': 974, 'faster': 975, 'soul': 976, 'consider': 977, 'rich': 978, 'serious': 979, 'jump': 980, 'felt': 981, 'difference': 982, 'attract': 983, 'hehehe': 984, 'prepare': 985, 'carry': 986, 'event': 987, 'button': 988, 'law': 989, 'government': 990, 'mention': 991, 'dry': 992, 'ill': 993, 'fag': 994, 'xx': 995, 'roast': 996, 'nd': 997, 'won': 998, 'figure': 999, 'complain': 1000, 'swim': 1001, 'anger': 1002, 'push': 1003, 'ugh': 1004, 'sale': 1005, 'rule': 1006, 'thursday': 1007, 'window': 1008, 'easily': 1009, 'project': 1010, 'personal': 1011, 'makeup': 1012, 'company': 1013, 'film': 1014, 'concert': 1015, 'highschool': 1016, 'remind': 1017, 'basically': 1018, 'pass': 1019, 'quick': 1020, 'didnt': 1021, 'league': 1022, 'pop': 1023, 'ring': 1024, 'afraid': 1025, 'cigarette': 1026, 'campus': 1027, 'warm': 1028, 'ive': 1029, 'period': 1030, 'biggest': 1031, 'space': 1032, 'costa': 1033, 'fly': 1034, 'ahead': 1035, 'throat': 1036, 'slow': 1037, 'intimidate': 1038, 'air': 1039, 'babe': 1040, 'situation': 1041, 'bottle': 1042, 'deep': 1043, 'opinion': 1044, 'cycle': 1045, 'red': 1046, 'along': 1047, 'often': 1048, 'alive': 1049, 'prove': 1050}\n",
            "1050\n",
            "{'stuff', 'someone', 'away', 'fun', 'voice', 'sound', 'iphone', 'sing', 'mum', 'dollar', 'both', 'cute', 'example', 'freak', 'concert', 'loud', 'credit', 'her', 'jerk', 'health', 'clothe', 'join', 'mom', 'begin', 'won', 'scream', 'important', 'turn', 'smh', 'listen', 'guy', 'from', 'worry', 'dinner', 'its', 'yes', 'doe', 'w', 'minute', 'd', 'ticket', 'high', 'parent', 'me', 'angry', 'person', 'sarcastic', 'easy', 'was', 'perfect', 'today', 'him', 'super', 'wide', 'story', 'lot', 'usually', 'alive', 'often', 'normal', 'cool', 'special', 'lie', 'win', 'blow', 'deal', 'fly', 'government', 'park', 'doubt', 'weekend', 'since', 'lab', 'day', 'em', 'annoy', 'hot', 'half', 'sign', 'smoke', 'under', 'police', 'side', 'keep', 'cry', 'jump', 'all', 'bit', 'whatever', 'b', 'people', 'so', 'fight', 'extra', 'video', 'smile', 'can', 'even', 'future', 'money', 'babysitting', 'until', 'that', 'longer', 'question', 'asleep', 'build', 'blast', 'party', 'which', 'pour', 'sense', 'big', 'pack', 'sale', 'female', 'everyone', 'break', 'reason', 'mouth', 'finish', 'part', 'creepy', 'amazin', 'enjoy', 'release', 'draw', 'around', 'good', 'highschool', 'fag', 'shoot', 'algebra', 'check', 'matter', 'goal', 'far', 'dress', 'fire', 'remember', 'prayer', 'bill', 'just', 'bitch', 'shout', 'month', 'pm', 'together', 'track', 'almost', 'retweet', 'frustration', 'whenever', 'touch', 'again', 'disappointment', 'fine', 'somebody', 'hospital', 'know', 'top', 'free', 'few', 'sell', 'throat', 'cigarette', 'happy', 'damn', 'later', 'XYZ', 'fat', 'anymore', 'kid', 'actually', 'their', 'understand', 'pull', 'middle', 'expect', 'reach', 'date', 'coffee', 'happen', 'word', 'game', 'might', 'death', 'promise', 'little', 'determine', 'christmas', 'law', 'license', 'dark', 'snow', 'agree', 'hurt', 'everybody', 'upset', 'group', 'plus', 'truly', 'up', 'shave', 'cover', 'ever', 'homework', 'hey', 'public', 'huge', 'youtube', 'bed', 'yay', 'your', 'worst', 'ride', 'piss', 'p', 'mine', 'soon', 'attract', 'luck', 'wow', 'order', 'anger', 'awesome', 'continue', 'burn', 'is', 'poor', 'worth', 'pretty', 'full', 'film', 'learn', 'remind', 'stand', 'fact', 'set', 'hilarious', 'o', 'bring', 'opinion', 'idk', 'babe', 'period', 'here', 'yourself', 'sleep', 'assignment', 'rain', 'she', 'view', 'negative', 'hello', 'train', 'arm', 'android', 'prove', 'business', 'fan', 'end', 'grass', 'bore', 'tv', 'finally', 'please', 'better', 'back', 'follower', 'n', 'best', 'god', 'weight', 'mention', 'take', 'google', 'virginity', 'ugly', 'close', 'return', 'least', 'dear', 'chance', 'yeah', 'when', 'fill', 'fail', 'awkward', 'ai', 'closer', 'okay', 'email', 'an', 'dick', 'apple', 'handle', 'home', 'saturday', 'met', 'bullshit', 'show', 'record', 'different', 'face', 'sure', 'see', 'song', 'dream', 'internet', 'to', 'over', 'live', 'limit', 'computer', 'account', 'between', 'whole', 'felt', 'decision', 'shitty', 'follow', 'band', 'care', 'at', 'entire', 'winter', 'tear', 'kill', 'should', 'as', 'week', 'lmao', 'lady', 'app', 'morning', 'explain', 'along', 'pray', 'wtf', 'watch', 'buy', 'food', 'nightmare', 'pop', 'bank', 'u', 'imagine', 'bro', 'marry', 'wish', 'button', 'save', 'once', 'test', 'greener', 'cancel', 'cant', 'bathroom', 'serve', 'awake', 'early', 'hand', 'mind', 'message', 'also', 'bunch', 'upstairs', 'class', 'tax', 'pain', 'sweet', 'these', 'xyz', 'lose', 'alarm', 'for', 'alone', 'leg', 'problem', 'without', 'hug', 'ok', 'beer', 'ignore', 's', 'may', 'teacher', 'off', 'number', 'star', 'much', 'lay', 'or', 'absolutely', 'point', 'delete', 'favorite', 'photo', 'water', 'smell', 'with', 'like', 'man', 'respect', 'deserve', 'ha', 'while', 'visit', 'facebook', 'other', 'house', 'equal', 'sometimes', 'behind', 'unless', 'catch', 'get', 'rock', 'except', 'friday', 'haha', 'attempt', 'family', 'human', 'say', 'outside', 'die', 'chemistry', 'not', 'project', 'create', 'gas', 'lunch', 'kick', 'air', 'argument', 'line', 'possible', 'dad', 'everything', 'laugh', 'each', 'black', 'had', 'hi', 'country', 'couple', 'letter', 'give', 'saw', 'yesterday', 'first', 'school', 'need', 'moment', 'bout', 'insult', 'mother', 'others', 'think', 'due', 'self', 'will', 'what', 'probably', 'completely', 'case', 'head', 'realize', 'play', 'hope', 'k', 'instagram', 'love', 'because', 'tea', 'add', 'door', 'three', 'blood', 'prepare', 'they', 'post', 'truth', 'pls', 'career', 'choice', 'make', 'more', 'forever', 'share', 'tuesday', 'pizza', 'difference', 'experience', 'street', 'kiss', 'hehehe', 'president', 'space', 'go', 'music', 'look', 'eat', 'suppose', 'has', 'talk', 'stupid', 'chill', 'sweat', 'bet', 'honestly', 'child', 'it', 'son', 'eye', 'trump', 'shirt', 'he', 'low', 't', 'min', 'page', 'come', 'gym', 'wet', 'lazy', 'though', 'small', 'by', 'fast', 'th', 'open', 'those', 'long', 'movie', 'wall', 'hahaha', 'than', 'monday', 'rest', 'summer', 'bottle', 'light', 'no', 'boy', 'about', 'yo', 'heart', 'and', 'im', 'weather', 'inside', 'believe', 'mr', 'new', 'seem', 'shower', 'throw', 'traffic', 'sport', 'am', 'one', 'cook', 'boyfriend', 'work', 'gay', 'pass', 'crazy', 'gorgeous', 'beautiful', 'forget', 'tire', 'night', 'could', 'another', 'course', 'drown', 'nobody', 'brain', 'a', 'never', 'car', 'justin', 'memory', 'shock', 'note', 'call', 'paper', 'million', 'fuck', 'city', 'via', 'pas', 'key', 'gotta', 'clean', 'move', 'sunday', 'friend', 'lil', 'figure', 'intimidate', 'ring', 'x', 'rn', 'type', 'exam', 'pay', 'holiday', 'hair', 'ready', 'r', 'interview', 'yell', 'cuz', 'straight', 'album', 'too', 'hour', 'hoe', 'finger', 'answer', 'dance', 'nap', 'f', 'sex', 'great', 'amuse', 'pic', 'short', 'sick', 'read', 'dry', 'mess', 'hide', 'drop', 'college', 'something', 'dont', 'help', 'scar', 'successful', 'team', 'stay', 'carry', 'pick', 'place', 'percent', 'idiot', 'power', 'where', 'cut', 'episode', 'service', 'plan', 'two', 'roll', 'rule', 'really', 'gonna', 'past', 'floor', 'way', 'bless', 'five', 'time', 'social', 'pretend', 'afraid', 'physic', 'spend', 'this', 'but', 'swim', 'fake', 'easily', 'shame', 'football', 'degree', 'busy', 'cause', 'elephant', 'card', 'able', 'everyday', 'act', 'argue', 'sit', 'find', 'impressive', 'wash', 'joy', 'his', 'event', 'tomorrow', 'interest', 'issue', 'oh', 'window', 'store', 'seriously', 'math', 'ahead', 'league', 'tweet', 'own', 'allow', 'welcome', 'instead', 'totally', 'situation', 'body', 'many', 'why', 'through', 'literally', 'news', 'update', 'girlfriend', 'ive', 'meet', 'art', 'rich', 'let', 'arrest', 'baby', 'before', 'online', 'my', 'lord', 'loose', 'taste', 'funny', 'our', 'enough', 'drug', 'bright', 'faster', 'xx', 'kinda', 'hard', 'fall', 'thats', 'freshly', 'old', 'bye', 'tonight', 'wonderful', 'season', 'dog', 'attractive', 'grade', 'send', 'white', 'them', 'direction', 'stress', 'company', 'how', 'shit', 'conversation', 'feel', 'young', 'bus', 'ago', 'match', 'next', 'kind', 'pant', 'block', 'warm', 'already', 'else', 'honest', 'right', 'ya', 'final', 'raise', 'any', 'mood', 'serious', 'schedule', 'ugh', 'list', 'lol', 'would', 'nothing', 'same', 'thanksgiving', 'practice', 'wonder', 'suck', 'girl', 'decide', 'ill', 'be', 'stop', 'surprise', 'still', 'have', 'into', 'last', 'foot', 'apparently', 'walk', 'such', 'well', 'snapchat', 'most', 'roast', 'nd', 'tattoo', 'leave', 'hop', 'till', 'tho', 'the', 'front', 'cat', 'hillary', 'adult', 'hungry', 'harry', 'hive', 'quick', 'butt', 'cough', 'mean', 'guess', 'attack', 'judge', 'birthday', 'gift', 'against', 'put', 'dude', 'american', 'level', 'ex', 'grow', 'definitely', 'every', 'absolute', 'ball', 'picture', 'men', 'babysit', 'write', 'charge', 'drink', 'trust', 'piece', 'success', 'hell', 'complain', 'brother', 'puke', 'office', 'ask', 'yet', 'beat', 'report', 'run', 'wife', 'mad', 'crush', 'wanna', 'of', 'club', 'player', 'download', 'anything', 'basically', 'shut', 'treat', 'sneeze', 'relationship', 'did', 'amaze', 'teach', 'nigga', 'bestfriend', 'do', 'if', 'glad', 'makeup', 'push', 'waste', 'wrong', 'goodnight', 'campus', 'weird', 'ass', 'become', 'doctor', 'happiness', 'especially', 'study', 'true', 'change', 'attention', 'miss', 'wait', 'deep', 'history', 'neighbor', 'america', 'hockey', 'costa', 'thursday', 'cold', 'text', 'tell', 'ruin', 'obama', 'shop', 'four', 'nice', 'bad', 'sorry', 'then', 'invest', 'some', 'swear', 'lead', 'terrible', 'us', 'out', 'i', 'step', 'safe', 'personal', 'y', 'only', 'phone', 'peace', 'english', 'rather', 'worse', 'exactly', 'world', 'want', 'year', 'second', 'sun', 'got', 'red', 'wake', 'vote', 'there', 'late', 'secret', 'lovely', 'smart', 'lucky', 'life', 'didnt', 'wear', 'clock', 'choose', 'drive', 'control', 'fridge', 'you', 'bc', 'dead', 'c', 'hate', 'thing', 'try', 'twitter', 'slow', 'appreciate', 'use', 'bag', 'very', 'proud', 'anyone', 'race', 'shift', 'in', 'sad', 'maybe', 'fix', 'quite', 'book', 'excite', 'forward', 'st', 'fantastic', 'subtle', 'real', 'convention', 'down', 'soul', 'strong', 'hang', 'road', 'idea', 'age', 'semester', 'character', 'stick', 'single', 'after', 'notice', 'biggest', 'mistake', 'room', 'are', 'comment', 'job', 'state', 'who', 'random', 'sore', 'hear', 'consider', 'either', 'reply', 'during', 'dumb', 'omg', 'myself', 'thank', 'trip', 'count', 'woman', 'm', 'always', 'must', 'hold', 'now', 'war', 'medium', 'cycle', 'name', 'student', 'speak', 'we', 'on', 'le', 'joke', 'support', 'start', 'bear', 'hit', 'sister', 'af'}\n",
            "1051\n",
            "[[  1   2 175 ...   0   0   0]\n",
            " [ 64 638  18 ...   0   0   0]\n",
            " [  1 103 112 ...   0   0   0]\n",
            " ...\n",
            " [  1   1  68 ...   0   0   0]\n",
            " [  1 371  27 ...   0   0   0]\n",
            " [  1   1  20 ...   0   0   0]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'able',\n",
              " 'about',\n",
              " 'absolute',\n",
              " 'absolutely',\n",
              " 'account',\n",
              " 'act',\n",
              " 'actually',\n",
              " 'add',\n",
              " 'adult',\n",
              " 'af',\n",
              " 'afraid',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'age',\n",
              " 'ago',\n",
              " 'agree',\n",
              " 'ahead',\n",
              " 'ai',\n",
              " 'air',\n",
              " 'alarm',\n",
              " 'album',\n",
              " 'algebra',\n",
              " 'alive',\n",
              " 'all',\n",
              " 'allow',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'already',\n",
              " 'also',\n",
              " 'always',\n",
              " 'am',\n",
              " 'amaze',\n",
              " 'amazin',\n",
              " 'america',\n",
              " 'american',\n",
              " 'amuse',\n",
              " 'an',\n",
              " 'and',\n",
              " 'android',\n",
              " 'anger',\n",
              " 'angry',\n",
              " 'annoy',\n",
              " 'another',\n",
              " 'answer',\n",
              " 'any',\n",
              " 'anymore',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'app',\n",
              " 'apparently',\n",
              " 'apple',\n",
              " 'appreciate',\n",
              " 'are',\n",
              " 'argue',\n",
              " 'argument',\n",
              " 'arm',\n",
              " 'around',\n",
              " 'arrest',\n",
              " 'art',\n",
              " 'as',\n",
              " 'ask',\n",
              " 'asleep',\n",
              " 'ass',\n",
              " 'assignment',\n",
              " 'at',\n",
              " 'attack',\n",
              " 'attempt',\n",
              " 'attention',\n",
              " 'attract',\n",
              " 'attractive',\n",
              " 'awake',\n",
              " 'away',\n",
              " 'awesome',\n",
              " 'awkward',\n",
              " 'b',\n",
              " 'babe',\n",
              " 'baby',\n",
              " 'babysit',\n",
              " 'babysitting',\n",
              " 'back',\n",
              " 'bad',\n",
              " 'bag',\n",
              " 'ball',\n",
              " 'band',\n",
              " 'bank',\n",
              " 'basically',\n",
              " 'bathroom',\n",
              " 'bc',\n",
              " 'be',\n",
              " 'bear',\n",
              " 'beat',\n",
              " 'beautiful',\n",
              " 'because',\n",
              " 'become',\n",
              " 'bed',\n",
              " 'beer',\n",
              " 'before',\n",
              " 'begin',\n",
              " 'behind',\n",
              " 'believe',\n",
              " 'best',\n",
              " 'bestfriend',\n",
              " 'bet',\n",
              " 'better',\n",
              " 'between',\n",
              " 'big',\n",
              " 'biggest',\n",
              " 'bill',\n",
              " 'birthday',\n",
              " 'bit',\n",
              " 'bitch',\n",
              " 'black',\n",
              " 'blast',\n",
              " 'bless',\n",
              " 'block',\n",
              " 'blood',\n",
              " 'blow',\n",
              " 'body',\n",
              " 'book',\n",
              " 'bore',\n",
              " 'both',\n",
              " 'bottle',\n",
              " 'bout',\n",
              " 'boy',\n",
              " 'boyfriend',\n",
              " 'brain',\n",
              " 'break',\n",
              " 'bright',\n",
              " 'bring',\n",
              " 'bro',\n",
              " 'brother',\n",
              " 'build',\n",
              " 'bullshit',\n",
              " 'bunch',\n",
              " 'burn',\n",
              " 'bus',\n",
              " 'business',\n",
              " 'busy',\n",
              " 'but',\n",
              " 'butt',\n",
              " 'button',\n",
              " 'buy',\n",
              " 'by',\n",
              " 'bye',\n",
              " 'c',\n",
              " 'call',\n",
              " 'campus',\n",
              " 'can',\n",
              " 'cancel',\n",
              " 'cant',\n",
              " 'car',\n",
              " 'card',\n",
              " 'care',\n",
              " 'career',\n",
              " 'carry',\n",
              " 'case',\n",
              " 'cat',\n",
              " 'catch',\n",
              " 'cause',\n",
              " 'chance',\n",
              " 'change',\n",
              " 'character',\n",
              " 'charge',\n",
              " 'check',\n",
              " 'chemistry',\n",
              " 'child',\n",
              " 'chill',\n",
              " 'choice',\n",
              " 'choose',\n",
              " 'christmas',\n",
              " 'cigarette',\n",
              " 'city',\n",
              " 'class',\n",
              " 'clean',\n",
              " 'clock',\n",
              " 'close',\n",
              " 'closer',\n",
              " 'clothe',\n",
              " 'club',\n",
              " 'coffee',\n",
              " 'cold',\n",
              " 'college',\n",
              " 'come',\n",
              " 'comment',\n",
              " 'company',\n",
              " 'complain',\n",
              " 'completely',\n",
              " 'computer',\n",
              " 'concert',\n",
              " 'consider',\n",
              " 'continue',\n",
              " 'control',\n",
              " 'convention',\n",
              " 'conversation',\n",
              " 'cook',\n",
              " 'cool',\n",
              " 'costa',\n",
              " 'cough',\n",
              " 'could',\n",
              " 'count',\n",
              " 'country',\n",
              " 'couple',\n",
              " 'course',\n",
              " 'cover',\n",
              " 'crazy',\n",
              " 'create',\n",
              " 'credit',\n",
              " 'creepy',\n",
              " 'crush',\n",
              " 'cry',\n",
              " 'cut',\n",
              " 'cute',\n",
              " 'cuz',\n",
              " 'cycle',\n",
              " 'd',\n",
              " 'dad',\n",
              " 'damn',\n",
              " 'dance',\n",
              " 'dark',\n",
              " 'date',\n",
              " 'day',\n",
              " 'dead',\n",
              " 'deal',\n",
              " 'dear',\n",
              " 'death',\n",
              " 'decide',\n",
              " 'decision',\n",
              " 'deep',\n",
              " 'definitely',\n",
              " 'degree',\n",
              " 'delete',\n",
              " 'deserve',\n",
              " 'determine',\n",
              " 'dick',\n",
              " 'did',\n",
              " 'didnt',\n",
              " 'die',\n",
              " 'difference',\n",
              " 'different',\n",
              " 'dinner',\n",
              " 'direction',\n",
              " 'disappointment',\n",
              " 'do',\n",
              " 'doctor',\n",
              " 'doe',\n",
              " 'dog',\n",
              " 'dollar',\n",
              " 'dont',\n",
              " 'door',\n",
              " 'doubt',\n",
              " 'down',\n",
              " 'download',\n",
              " 'draw',\n",
              " 'dream',\n",
              " 'dress',\n",
              " 'drink',\n",
              " 'drive',\n",
              " 'drop',\n",
              " 'drown',\n",
              " 'drug',\n",
              " 'dry',\n",
              " 'dude',\n",
              " 'due',\n",
              " 'dumb',\n",
              " 'during',\n",
              " 'each',\n",
              " 'early',\n",
              " 'easily',\n",
              " 'easy',\n",
              " 'eat',\n",
              " 'either',\n",
              " 'elephant',\n",
              " 'else',\n",
              " 'em',\n",
              " 'email',\n",
              " 'end',\n",
              " 'english',\n",
              " 'enjoy',\n",
              " 'enough',\n",
              " 'entire',\n",
              " 'episode',\n",
              " 'equal',\n",
              " 'especially',\n",
              " 'even',\n",
              " 'event',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everybody',\n",
              " 'everyday',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'ex',\n",
              " 'exactly',\n",
              " 'exam',\n",
              " 'example',\n",
              " 'except',\n",
              " 'excite',\n",
              " 'expect',\n",
              " 'experience',\n",
              " 'explain',\n",
              " 'extra',\n",
              " 'eye',\n",
              " 'f',\n",
              " 'face',\n",
              " 'facebook',\n",
              " 'fact',\n",
              " 'fag',\n",
              " 'fail',\n",
              " 'fake',\n",
              " 'fall',\n",
              " 'family',\n",
              " 'fan',\n",
              " 'fantastic',\n",
              " 'far',\n",
              " 'fast',\n",
              " 'faster',\n",
              " 'fat',\n",
              " 'favorite',\n",
              " 'feel',\n",
              " 'felt',\n",
              " 'female',\n",
              " 'few',\n",
              " 'fight',\n",
              " 'figure',\n",
              " 'fill',\n",
              " 'film',\n",
              " 'final',\n",
              " 'finally',\n",
              " 'find',\n",
              " 'fine',\n",
              " 'finger',\n",
              " 'finish',\n",
              " 'fire',\n",
              " 'first',\n",
              " 'five',\n",
              " 'fix',\n",
              " 'floor',\n",
              " 'fly',\n",
              " 'follow',\n",
              " 'follower',\n",
              " 'food',\n",
              " 'foot',\n",
              " 'football',\n",
              " 'for',\n",
              " 'forever',\n",
              " 'forget',\n",
              " 'forward',\n",
              " 'four',\n",
              " 'freak',\n",
              " 'free',\n",
              " 'freshly',\n",
              " 'friday',\n",
              " 'fridge',\n",
              " 'friend',\n",
              " 'from',\n",
              " 'front',\n",
              " 'frustration',\n",
              " 'fuck',\n",
              " 'full',\n",
              " 'fun',\n",
              " 'funny',\n",
              " 'future',\n",
              " 'game',\n",
              " 'gas',\n",
              " 'gay',\n",
              " 'get',\n",
              " 'gift',\n",
              " 'girl',\n",
              " 'girlfriend',\n",
              " 'give',\n",
              " 'glad',\n",
              " 'go',\n",
              " 'goal',\n",
              " 'god',\n",
              " 'gonna',\n",
              " 'good',\n",
              " 'goodnight',\n",
              " 'google',\n",
              " 'gorgeous',\n",
              " 'got',\n",
              " 'gotta',\n",
              " 'government',\n",
              " 'grade',\n",
              " 'grass',\n",
              " 'great',\n",
              " 'greener',\n",
              " 'group',\n",
              " 'grow',\n",
              " 'guess',\n",
              " 'guy',\n",
              " 'gym',\n",
              " 'ha',\n",
              " 'had',\n",
              " 'haha',\n",
              " 'hahaha',\n",
              " 'hair',\n",
              " 'half',\n",
              " 'hand',\n",
              " 'handle',\n",
              " 'hang',\n",
              " 'happen',\n",
              " 'happiness',\n",
              " 'happy',\n",
              " 'hard',\n",
              " 'harry',\n",
              " 'has',\n",
              " 'hate',\n",
              " 'have',\n",
              " 'he',\n",
              " 'head',\n",
              " 'health',\n",
              " 'hear',\n",
              " 'heart',\n",
              " 'hehehe',\n",
              " 'hell',\n",
              " 'hello',\n",
              " 'help',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hey',\n",
              " 'hi',\n",
              " 'hide',\n",
              " 'high',\n",
              " 'highschool',\n",
              " 'hilarious',\n",
              " 'hillary',\n",
              " 'him',\n",
              " 'his',\n",
              " 'history',\n",
              " 'hit',\n",
              " 'hive',\n",
              " 'hockey',\n",
              " 'hoe',\n",
              " 'hold',\n",
              " 'holiday',\n",
              " 'home',\n",
              " 'homework',\n",
              " 'honest',\n",
              " 'honestly',\n",
              " 'hop',\n",
              " 'hope',\n",
              " 'hospital',\n",
              " 'hot',\n",
              " 'hour',\n",
              " 'house',\n",
              " 'how',\n",
              " 'hug',\n",
              " 'huge',\n",
              " 'human',\n",
              " 'hungry',\n",
              " 'hurt',\n",
              " 'i',\n",
              " 'idea',\n",
              " 'idiot',\n",
              " 'idk',\n",
              " 'if',\n",
              " 'ignore',\n",
              " 'ill',\n",
              " 'im',\n",
              " 'imagine',\n",
              " 'important',\n",
              " 'impressive',\n",
              " 'in',\n",
              " 'inside',\n",
              " 'instagram',\n",
              " 'instead',\n",
              " 'insult',\n",
              " 'interest',\n",
              " 'internet',\n",
              " 'interview',\n",
              " 'intimidate',\n",
              " 'into',\n",
              " 'invest',\n",
              " 'iphone',\n",
              " 'is',\n",
              " 'issue',\n",
              " 'it',\n",
              " 'its',\n",
              " 'ive',\n",
              " 'jerk',\n",
              " 'job',\n",
              " 'join',\n",
              " 'joke',\n",
              " 'joy',\n",
              " 'judge',\n",
              " 'jump',\n",
              " 'just',\n",
              " 'justin',\n",
              " 'k',\n",
              " 'keep',\n",
              " 'key',\n",
              " 'kick',\n",
              " 'kid',\n",
              " 'kill',\n",
              " 'kind',\n",
              " 'kinda',\n",
              " 'kiss',\n",
              " 'know',\n",
              " 'lab',\n",
              " 'lady',\n",
              " 'last',\n",
              " 'late',\n",
              " 'later',\n",
              " 'laugh',\n",
              " 'law',\n",
              " 'lay',\n",
              " 'lazy',\n",
              " 'le',\n",
              " 'lead',\n",
              " 'league',\n",
              " 'learn',\n",
              " 'least',\n",
              " 'leave',\n",
              " 'leg',\n",
              " 'let',\n",
              " 'letter',\n",
              " 'level',\n",
              " 'license',\n",
              " 'lie',\n",
              " 'life',\n",
              " 'light',\n",
              " 'like',\n",
              " 'lil',\n",
              " 'limit',\n",
              " 'line',\n",
              " 'list',\n",
              " 'listen',\n",
              " 'literally',\n",
              " 'little',\n",
              " 'live',\n",
              " 'lmao',\n",
              " 'lol',\n",
              " 'long',\n",
              " 'longer',\n",
              " 'look',\n",
              " 'loose',\n",
              " 'lord',\n",
              " 'lose',\n",
              " 'lot',\n",
              " 'loud',\n",
              " 'love',\n",
              " 'lovely',\n",
              " 'low',\n",
              " 'luck',\n",
              " 'lucky',\n",
              " 'lunch',\n",
              " 'm',\n",
              " 'mad',\n",
              " 'make',\n",
              " 'makeup',\n",
              " 'man',\n",
              " 'many',\n",
              " 'marry',\n",
              " 'match',\n",
              " 'math',\n",
              " 'matter',\n",
              " 'may',\n",
              " 'maybe',\n",
              " 'me',\n",
              " 'mean',\n",
              " 'medium',\n",
              " 'meet',\n",
              " 'memory',\n",
              " 'men',\n",
              " 'mention',\n",
              " 'mess',\n",
              " 'message',\n",
              " 'met',\n",
              " 'middle',\n",
              " 'might',\n",
              " 'million',\n",
              " 'min',\n",
              " 'mind',\n",
              " 'mine',\n",
              " 'minute',\n",
              " 'miss',\n",
              " 'mistake',\n",
              " 'mom',\n",
              " 'moment',\n",
              " 'monday',\n",
              " 'money',\n",
              " 'month',\n",
              " 'mood',\n",
              " 'more',\n",
              " 'morning',\n",
              " 'most',\n",
              " 'mother',\n",
              " 'mouth',\n",
              " 'move',\n",
              " 'movie',\n",
              " 'mr',\n",
              " 'much',\n",
              " 'mum',\n",
              " 'music',\n",
              " 'must',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'n',\n",
              " 'name',\n",
              " 'nap',\n",
              " 'nd',\n",
              " 'need',\n",
              " 'negative',\n",
              " 'neighbor',\n",
              " 'never',\n",
              " 'new',\n",
              " 'news',\n",
              " 'next',\n",
              " 'nice',\n",
              " 'nigga',\n",
              " 'night',\n",
              " 'nightmare',\n",
              " 'no',\n",
              " 'nobody',\n",
              " 'normal',\n",
              " 'not',\n",
              " 'note',\n",
              " 'nothing',\n",
              " 'notice',\n",
              " 'now',\n",
              " 'number',\n",
              " 'o',\n",
              " 'obama',\n",
              " 'of',\n",
              " 'off',\n",
              " 'office',\n",
              " 'often',\n",
              " 'oh',\n",
              " 'ok',\n",
              " 'okay',\n",
              " 'old',\n",
              " 'omg',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " 'online',\n",
              " 'only',\n",
              " 'open',\n",
              " 'opinion',\n",
              " 'or',\n",
              " 'order',\n",
              " 'other',\n",
              " 'others',\n",
              " 'our',\n",
              " 'out',\n",
              " 'outside',\n",
              " 'over',\n",
              " 'own',\n",
              " 'p',\n",
              " 'pack',\n",
              " 'page',\n",
              " 'pain',\n",
              " 'pant',\n",
              " 'paper',\n",
              " 'parent',\n",
              " 'park',\n",
              " 'part',\n",
              " 'party',\n",
              " 'pas',\n",
              " 'pass',\n",
              " 'past',\n",
              " 'pay',\n",
              " 'peace',\n",
              " 'people',\n",
              " 'percent',\n",
              " 'perfect',\n",
              " 'period',\n",
              " 'person',\n",
              " 'personal',\n",
              " 'phone',\n",
              " 'photo',\n",
              " 'physic',\n",
              " 'pic',\n",
              " 'pick',\n",
              " 'picture',\n",
              " 'piece',\n",
              " 'piss',\n",
              " 'pizza',\n",
              " 'place',\n",
              " 'plan',\n",
              " 'play',\n",
              " 'player',\n",
              " 'please',\n",
              " 'pls',\n",
              " 'plus',\n",
              " 'pm',\n",
              " 'point',\n",
              " 'police',\n",
              " 'poor',\n",
              " 'pop',\n",
              " 'possible',\n",
              " 'post',\n",
              " 'pour',\n",
              " 'power',\n",
              " 'practice',\n",
              " 'pray',\n",
              " 'prayer',\n",
              " 'prepare',\n",
              " 'president',\n",
              " 'pretend',\n",
              " 'pretty',\n",
              " 'probably',\n",
              " 'problem',\n",
              " 'project',\n",
              " 'promise',\n",
              " 'proud',\n",
              " 'prove',\n",
              " 'public',\n",
              " 'puke',\n",
              " 'pull',\n",
              " 'push',\n",
              " 'put',\n",
              " 'question',\n",
              " 'quick',\n",
              " 'quite',\n",
              " 'r',\n",
              " 'race',\n",
              " 'rain',\n",
              " 'raise',\n",
              " 'random',\n",
              " 'rather',\n",
              " 'reach',\n",
              " 'read',\n",
              " 'ready',\n",
              " 'real',\n",
              " 'realize',\n",
              " 'really',\n",
              " 'reason',\n",
              " 'record',\n",
              " 'red',\n",
              " 'relationship',\n",
              " 'release',\n",
              " 'remember',\n",
              " 'remind',\n",
              " 'reply',\n",
              " 'report',\n",
              " 'respect',\n",
              " 'rest',\n",
              " 'return',\n",
              " 'retweet',\n",
              " 'rich',\n",
              " 'ride',\n",
              " 'right',\n",
              " 'ring',\n",
              " 'rn',\n",
              " 'road',\n",
              " 'roast',\n",
              " 'rock',\n",
              " 'roll',\n",
              " 'room',\n",
              " 'ruin',\n",
              " 'rule',\n",
              " 'run',\n",
              " 's',\n",
              " 'sad',\n",
              " 'safe',\n",
              " 'sale',\n",
              " 'same',\n",
              " 'sarcastic',\n",
              " 'saturday',\n",
              " 'save',\n",
              " 'saw',\n",
              " 'say',\n",
              " 'scar',\n",
              " 'schedule',\n",
              " 'school',\n",
              " 'scream',\n",
              " 'season',\n",
              " 'second',\n",
              " 'secret',\n",
              " 'see',\n",
              " 'seem',\n",
              " 'self',\n",
              " 'sell',\n",
              " 'semester',\n",
              " 'send',\n",
              " 'sense',\n",
              " 'serious',\n",
              " 'seriously',\n",
              " 'serve',\n",
              " 'service',\n",
              " 'set',\n",
              " 'sex',\n",
              " 'shame',\n",
              " 'share',\n",
              " 'shave',\n",
              " 'she',\n",
              " 'shift',\n",
              " 'shirt',\n",
              " 'shit',\n",
              " 'shitty',\n",
              " 'shock',\n",
              " 'shoot',\n",
              " 'shop',\n",
              " 'short',\n",
              " 'should',\n",
              " 'shout',\n",
              " 'show',\n",
              " 'shower',\n",
              " 'shut',\n",
              " 'sick',\n",
              " 'side',\n",
              " 'sign',\n",
              " 'since',\n",
              " 'sing',\n",
              " 'single',\n",
              " 'sister',\n",
              " 'sit',\n",
              " 'situation',\n",
              " 'sleep',\n",
              " 'slow',\n",
              " 'small',\n",
              " 'smart',\n",
              " 'smell',\n",
              " 'smh',\n",
              " 'smile',\n",
              " 'smoke',\n",
              " 'snapchat',\n",
              " 'sneeze',\n",
              " 'snow',\n",
              " 'so',\n",
              " 'social',\n",
              " 'some',\n",
              " 'somebody',\n",
              " 'someone',\n",
              " 'something',\n",
              " 'sometimes',\n",
              " 'son',\n",
              " 'song',\n",
              " 'soon',\n",
              " 'sore',\n",
              " 'sorry',\n",
              " 'soul',\n",
              " 'sound',\n",
              " 'space',\n",
              " 'speak',\n",
              " 'special',\n",
              " 'spend',\n",
              " 'sport',\n",
              " 'st',\n",
              " 'stand',\n",
              " 'star',\n",
              " 'start',\n",
              " 'state',\n",
              " 'stay',\n",
              " 'step',\n",
              " 'stick',\n",
              " 'still',\n",
              " 'stop',\n",
              " 'store',\n",
              " 'story',\n",
              " 'straight',\n",
              " 'street',\n",
              " 'stress',\n",
              " 'strong',\n",
              " 'student',\n",
              " 'study',\n",
              " 'stuff',\n",
              " 'stupid',\n",
              " 'subtle',\n",
              " 'success',\n",
              " 'successful',\n",
              " 'such',\n",
              " 'suck',\n",
              " 'summer',\n",
              " 'sun',\n",
              " 'sunday',\n",
              " 'super',\n",
              " 'support',\n",
              " 'suppose',\n",
              " 'sure',\n",
              " 'surprise',\n",
              " 'swear',\n",
              " 'sweat',\n",
              " 'sweet',\n",
              " 'swim',\n",
              " 't',\n",
              " 'take',\n",
              " 'talk',\n",
              " 'taste',\n",
              " 'tattoo',\n",
              " 'tax',\n",
              " 'tea',\n",
              " 'teach',\n",
              " 'teacher',\n",
              " 'team',\n",
              " 'tear',\n",
              " 'tell',\n",
              " 'terrible',\n",
              " 'test',\n",
              " 'text',\n",
              " 'th',\n",
              " 'than',\n",
              " 'thank',\n",
              " 'thanksgiving',\n",
              " 'that',\n",
              " 'thats',\n",
              " 'the',\n",
              " 'their',\n",
              " 'them',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'thing',\n",
              " 'think',\n",
              " 'this',\n",
              " 'tho',\n",
              " 'those',\n",
              " 'though',\n",
              " 'three',\n",
              " 'throat',\n",
              " 'through',\n",
              " 'throw',\n",
              " 'thursday',\n",
              " 'ticket',\n",
              " 'till',\n",
              " 'time',\n",
              " 'tire',\n",
              " 'to',\n",
              " 'today',\n",
              " 'together',\n",
              " 'tomorrow',\n",
              " 'tonight',\n",
              " 'too',\n",
              " 'top',\n",
              " 'totally',\n",
              " 'touch',\n",
              " 'track',\n",
              " 'traffic',\n",
              " 'train',\n",
              " 'treat',\n",
              " 'trip',\n",
              " 'true',\n",
              " 'truly',\n",
              " 'trump',\n",
              " 'trust',\n",
              " 'truth',\n",
              " 'try',\n",
              " 'tuesday',\n",
              " 'turn',\n",
              " 'tv',\n",
              " 'tweet',\n",
              " 'twitter',\n",
              " 'two',\n",
              " 'type',\n",
              " 'u',\n",
              " 'ugh',\n",
              " 'ugly',\n",
              " 'under',\n",
              " 'understand',\n",
              " 'unless',\n",
              " 'until',\n",
              " 'up',\n",
              " 'update',\n",
              " 'upset',\n",
              " 'upstairs',\n",
              " 'us',\n",
              " 'use',\n",
              " 'usually',\n",
              " 'very',\n",
              " 'via',\n",
              " 'video',\n",
              " 'view',\n",
              " 'virginity',\n",
              " 'visit',\n",
              " 'voice',\n",
              " 'vote',\n",
              " 'w',\n",
              " 'wait',\n",
              " 'wake',\n",
              " 'walk',\n",
              " 'wall',\n",
              " 'wanna',\n",
              " 'want',\n",
              " 'war',\n",
              " 'warm',\n",
              " 'was',\n",
              " 'wash',\n",
              " 'waste',\n",
              " 'watch',\n",
              " 'water',\n",
              " 'way',\n",
              " 'we',\n",
              " 'wear',\n",
              " 'weather',\n",
              " 'week',\n",
              " 'weekend',\n",
              " 'weight',\n",
              " 'weird',\n",
              " 'welcome',\n",
              " 'well',\n",
              " 'wet',\n",
              " 'what',\n",
              " 'whatever',\n",
              " 'when',\n",
              " 'whenever',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xam-hyX5JH28"
      },
      "source": [
        "Glove embedding file is used to map all top words that we have extracted with the words in glove embedding file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0lbhFSnF_CL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "540c02fc-fde9-43ed-9875-fdb4d56002fe"
      },
      "source": [
        "def get_embedding():\n",
        "        embeddings_index = {}\n",
        "        f = open(EMBEDDING_FILE)\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            if len(values) == EMBEDDING_DIM + 1 and word in top_words:\n",
        "                coefs = np.asarray(values[1:], dtype=\"float32\")\n",
        "                embeddings_index[word] = coefs\n",
        "        f.close()\n",
        "        return embeddings_index\n",
        "\n",
        "embeddings_index = get_embedding()\n",
        "print(\"Words are not found in the embedding:\", top_words - embeddings_index.keys())\n",
        "top_words = embeddings_index.keys()\n",
        "\n",
        "nb_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))   #### creating embedding matrix \n",
        "print(EMBEDDING_DIM)\n",
        "\n",
        "for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words are not found in the embedding: set()\n",
            "300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCOY_PDZReKo"
      },
      "source": [
        "+ve and -ve words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCvkPR5mRaLH"
      },
      "source": [
        "### Dont try this code,as the sentiment polarity is later detected by using DNN and LSTM...\n",
        "### This was the initial step taken just in order to predict whether sentiment analysis can be done or not.Later DNNs and LSTM proved to be great tool for this.\n",
        "\n",
        "\n",
        "\n",
        "positive_words = 'drive/My Drive/Colab Notebooks/positive_words.csv' \n",
        "positive_words_df = pd.read_csv(positive_words)\n",
        "positive_words_list = set(positive_words_df['positive_words'].apply(cutter))\n",
        "\n",
        "negative_words = 'drive/My Drive/Colab Notebooks/negative_words.csv'\n",
        "negative_words_df = pd.read_csv(negative_words)\n",
        "negative_words_list = set(negative_words_df['negative_words'].apply(cutter))\n",
        "\n",
        "df['neg_count'] = df['w_tokenized'].apply(lambda x: len([x for x in x.split() if x in negative_words_list]))\n",
        "df['pos_count'] = df['w_tokenized'].apply(lambda x: len([x for x in x.split() if x in positive_words_list]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuVM3E_dKQ7J"
      },
      "source": [
        "Splitting the dataset into 3 parts : training,validation and testing. 80% of dataset for training ,10%  for validation and 10% for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fNniOHejPN1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "dfa69c1c-fecb-486b-a1d9-eda6e7252faa"
      },
      "source": [
        "label=df['label']\n",
        "train_data = text_data[:5000]\n",
        "train_label = label[:5000]\n",
        "test_data= text_data[5001:7000]\n",
        "test_label=label[5001:7000]\n",
        "#train_data = text_data[:29138]\n",
        "#train_label = label[:29138]\n",
        "#val_data=text_data[29139:32780]\n",
        "#val_label=label[29139:32780]\n",
        "#test_data=text_data[32781:36423]\n",
        "#test_label=label[32781:36423]\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(test_label.shape)\n",
        "print(label.shape)\n",
        "print(text_data.shape)\n",
        "#print(val_data.shape)\n",
        "train_label.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 50)\n",
            "(1999, 50)\n",
            "(1999,)\n",
            "(36423,)\n",
            "(36423, 50)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5WRh3jfKrOo"
      },
      "source": [
        "Neural Networks. Usage of LSTM and Bidirectional,as we need to analyse text from both ends to increase the accuracy of detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcooWFqF_1av",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "8476f84e-3fa1-437e-ce85-c535d8d6c1d0"
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Embedding, Bidirectional, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
        "from keras.layers import LSTM, Input, BatchNormalization, Dropout, Activation,Flatten\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.wrappers.scikit_learn import KerasClassifier  ## Scikit-Learn classifier interface for tuning model parameters and fitting parameters\n",
        "from keras import regularizers\n",
        "model3=Sequential()\n",
        "emb_input=Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
        "embeddings=Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH, trainable=False)(emb_input)\n",
        "lstm_layer = Bidirectional(LSTM((32),input_shape=(None,50),return_sequences=False, recurrent_dropout=0.7, dropout=0.7))(embeddings)  \n",
        "output=Dense(16,kernel_regularizer=regularizers.l2(0.001),activation='elu')(lstm_layer)\n",
        "output=keras.layers.Dropout(0.4)(output)\n",
        "output = BatchNormalization()(output)             #### to avoid huge variance in weights obtained from prev layers.\n",
        "output=Dense(8,kernel_regularizer=regularizers.l2(0.001), activation='elu')(output)\n",
        "output=keras.layers.Dropout(0.4)(output)\n",
        "output = BatchNormalization()(output)\n",
        "output=Dense(4,kernel_regularizer=regularizers.l2(0.001), activation='elu')(output)\n",
        "output=keras.layers.Dropout(0.4)(output)\n",
        "output=Dense(1,activation='sigmoid')(output)      #### sigmoid is used as only 0 or 1 needs to be predicted in last layer of deep neural network.\n",
        "model3 = Model(inputs=[emb_input], outputs=[output])\n",
        "#keras.optimizers.Adam(lr=0.12, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model3.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model3.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_10 (InputLayer)        (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "embedding_10 (Embedding)     (None, 50, 300)           315300    \n",
            "_________________________________________________________________\n",
            "bidirectional_10 (Bidirectio (None, 64)                85248     \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 16)                1040      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 16)                64        \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8)                 0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 8)                 32        \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 4)                 36        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4)                 0         \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 1)                 5         \n",
            "=================================================================\n",
            "Total params: 401,861\n",
            "Trainable params: 86,513\n",
            "Non-trainable params: 315,348\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEYhVF1ll_Kt"
      },
      "source": [
        "**OVERFITTIG removed, after tuning hyperparameters and recognizing the best optimal values of hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moStCYsCLF5Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "4d9cbed1-edb4-4187-abab-e2d0fd5511a3"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
        "history=model3.fit(train_data,train_label,epochs=10,validation_split=0.1,batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4500 samples, validate on 500 samples\n",
            "Epoch 1/10\n",
            "4500/4500 [==============================] - 7s 1ms/step - loss: 0.7364 - acc: 0.5578 - val_loss: 0.7488 - val_acc: 0.3920\n",
            "Epoch 2/10\n",
            "4500/4500 [==============================] - 2s 355us/step - loss: 0.7038 - acc: 0.5820 - val_loss: 0.7683 - val_acc: 0.2840\n",
            "Epoch 3/10\n",
            "4500/4500 [==============================] - 2s 351us/step - loss: 0.6792 - acc: 0.6071 - val_loss: 0.7573 - val_acc: 0.3620\n",
            "Epoch 4/10\n",
            "4500/4500 [==============================] - 2s 353us/step - loss: 0.6573 - acc: 0.6307 - val_loss: 0.7093 - val_acc: 0.6440\n",
            "Epoch 5/10\n",
            "4500/4500 [==============================] - 2s 347us/step - loss: 0.6327 - acc: 0.6656 - val_loss: 0.6680 - val_acc: 0.8940\n",
            "Epoch 6/10\n",
            "4500/4500 [==============================] - 2s 338us/step - loss: 0.6170 - acc: 0.6796 - val_loss: 0.6340 - val_acc: 0.9700\n",
            "Epoch 7/10\n",
            "4500/4500 [==============================] - 2s 349us/step - loss: 0.5964 - acc: 0.6893 - val_loss: 0.6029 - val_acc: 0.9920\n",
            "Epoch 8/10\n",
            "4500/4500 [==============================] - 2s 354us/step - loss: 0.5789 - acc: 0.7182 - val_loss: 0.5728 - val_acc: 0.9980\n",
            "Epoch 9/10\n",
            "4500/4500 [==============================] - 2s 351us/step - loss: 0.5645 - acc: 0.7398 - val_loss: 0.5351 - val_acc: 1.0000\n",
            "Epoch 10/10\n",
            "4500/4500 [==============================] - 2s 346us/step - loss: 0.5463 - acc: 0.7531 - val_loss: 0.4916 - val_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGHQ5OptmGy0"
      },
      "source": [
        "Hypertuning Prameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5_68OSdNGkg"
      },
      "source": [
        "Using Keras Model in scikit learn : KerasClassifier.It creates ans returns a model as build_fn argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaWJFfgwMJFS"
      },
      "source": [
        "def c_model():\n",
        "              model1=Sequential()\n",
        "              emb_input=Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
        "              embeddings=Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH, trainable=False)(emb_input)\n",
        "              lstm_layer = Bidirectional(LSTM((32),input_shape=(None,50),return_sequences=False, recurrent_dropout=0.5, dropout=0.5))(embeddings)  \n",
        "              output=Dense(16,kernel_regularizer=regularizers.l2(0.001), activation='elu')(lstm_layer)\n",
        "              output=keras.layers.Dropout(0.4)(output)\n",
        "              output = BatchNormalization()(output)             #### to avoid huge variance in weights obtained from prev layers.\n",
        "              output=Dense(8,kernel_regularizer=regularizers.l2(0.001), activation='elu')(output)\n",
        "              output=keras.layers.Dropout(0.4)(output)\n",
        "              output = BatchNormalization()(output)\n",
        "              output=Dense(4,kernel_regularizer=regularizers.l2(0.001), activation='elu')(output)\n",
        "              output=keras.layers.Dropout(0.4)(output)\n",
        "              output=Dense(1,activation='sigmoid')(output)      #### sigmoid is used as only 0 or 1 needs to be predicted in last layer of deep neural network.\n",
        "              model1 = Model(inputs=[emb_input], outputs=[output])\n",
        "              model1.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "              return model1\n",
        "model2=KerasClassifier(build_fn=c_model,epochs=10,batch_size=512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtY9Ny1JvgxV"
      },
      "source": [
        "Training process.EarlyStopping is used to prevent overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEzhfICiIq7V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "beafc2f0-933f-49c3-bd0e-5d8a5d2210a5"
      },
      "source": [
        "#from keras.callbacks import EarlyStopping\n",
        "#early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
        "history1=model2.fit(train_data,train_label,epochs=10,validation_split=0.1,batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4500 samples, validate on 500 samples\n",
            "Epoch 1/10\n",
            "4500/4500 [==============================] - 14s 3ms/step - loss: 1.1302 - acc: 0.5504 - val_loss: 0.5191 - val_acc: 0.7960\n",
            "Epoch 2/10\n",
            "4500/4500 [==============================] - 2s 353us/step - loss: 1.0170 - acc: 0.5671 - val_loss: 0.7011 - val_acc: 0.6280\n",
            "Epoch 3/10\n",
            "4500/4500 [==============================] - 2s 342us/step - loss: 0.9283 - acc: 0.5904 - val_loss: 0.6505 - val_acc: 0.6580\n",
            "Epoch 4/10\n",
            "4500/4500 [==============================] - 2s 353us/step - loss: 0.8560 - acc: 0.6260 - val_loss: 0.5485 - val_acc: 0.7600\n",
            "Epoch 5/10\n",
            "4500/4500 [==============================] - 2s 358us/step - loss: 0.8209 - acc: 0.6391 - val_loss: 0.4588 - val_acc: 0.8640\n",
            "Epoch 6/10\n",
            "4500/4500 [==============================] - 2s 353us/step - loss: 0.7763 - acc: 0.6631 - val_loss: 0.4016 - val_acc: 0.8940\n",
            "Epoch 7/10\n",
            "4500/4500 [==============================] - 2s 352us/step - loss: 0.7381 - acc: 0.6751 - val_loss: 0.3824 - val_acc: 0.9160\n",
            "Epoch 8/10\n",
            "4500/4500 [==============================] - 2s 343us/step - loss: 0.6928 - acc: 0.7071 - val_loss: 0.3641 - val_acc: 0.9320\n",
            "Epoch 9/10\n",
            "4500/4500 [==============================] - 2s 359us/step - loss: 0.6942 - acc: 0.7036 - val_loss: 0.3388 - val_acc: 0.9520\n",
            "Epoch 10/10\n",
            "4500/4500 [==============================] - 2s 357us/step - loss: 0.6519 - acc: 0.7222 - val_loss: 0.3356 - val_acc: 0.9500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwOpcAqtZelF"
      },
      "source": [
        "Predicting test data and evaluating its accuracy and loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz0FeJ0ZBuuB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "407c35fa-366a-4d1a-d67e-1f83afda70e4"
      },
      "source": [
        "print(model1.predict(test_data))\n",
        "score = model1.evaluate(test_data,test_label, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "# so in the dataset ... 0 is non sarcastic and 1 is sarcastic."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00035664]\n",
            " [0.00028735]\n",
            " [0.0002833 ]\n",
            " ...\n",
            " [0.00037694]\n",
            " [0.00037481]\n",
            " [0.00037692]]\n",
            "Test loss: 0.00030539172750805924\n",
            "Test accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dkEN-tawmQ8"
      },
      "source": [
        "Trying different batch_sizes and epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rcef---pySyJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "355b04c8-d8ef-43e0-fbf3-16fe1cfbe15d"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "model2=KerasClassifier(build_fn=c_model)\n",
        "batch_sizes=[256,512]\n",
        "epochs=[10]\n",
        "parameters={'batch_size':batch_sizes,'epochs':epochs}\n",
        "clf=GridSearchCV(estimator=model2,param_grid=parameters,scoring=None,refit=True)\n",
        "clf.fit(train_data,train_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3333/3333 [==============================] - 14s 4ms/step - loss: 0.9197 - acc: 0.5347\n",
            "Epoch 2/10\n",
            "3333/3333 [==============================] - 2s 705us/step - loss: 0.8514 - acc: 0.5914\n",
            "Epoch 3/10\n",
            "3333/3333 [==============================] - 2s 709us/step - loss: 0.7961 - acc: 0.6496\n",
            "Epoch 4/10\n",
            "3333/3333 [==============================] - 2s 695us/step - loss: 0.7447 - acc: 0.6904\n",
            "Epoch 5/10\n",
            "3333/3333 [==============================] - 2s 691us/step - loss: 0.7135 - acc: 0.7285\n",
            "Epoch 6/10\n",
            "3333/3333 [==============================] - 2s 707us/step - loss: 0.6836 - acc: 0.7459\n",
            "Epoch 7/10\n",
            "3333/3333 [==============================] - 2s 703us/step - loss: 0.6576 - acc: 0.7783\n",
            "Epoch 8/10\n",
            "3333/3333 [==============================] - 2s 681us/step - loss: 0.6302 - acc: 0.8014\n",
            "Epoch 9/10\n",
            "3333/3333 [==============================] - 2s 690us/step - loss: 0.6053 - acc: 0.8236\n",
            "Epoch 10/10\n",
            "3333/3333 [==============================] - 2s 699us/step - loss: 0.5769 - acc: 0.8542\n",
            "1667/1667 [==============================] - 5s 3ms/step\n",
            "Epoch 1/10\n",
            "3333/3333 [==============================] - 14s 4ms/step - loss: 0.9353 - acc: 0.4404\n",
            "Epoch 2/10\n",
            "3333/3333 [==============================] - 2s 687us/step - loss: 0.8657 - acc: 0.4809\n",
            "Epoch 3/10\n",
            "3333/3333 [==============================] - 2s 701us/step - loss: 0.7891 - acc: 0.5437\n",
            "Epoch 4/10\n",
            "3333/3333 [==============================] - 2s 702us/step - loss: 0.7560 - acc: 0.5770\n",
            "Epoch 5/10\n",
            "3333/3333 [==============================] - 2s 691us/step - loss: 0.7093 - acc: 0.6331\n",
            "Epoch 6/10\n",
            "3333/3333 [==============================] - 2s 706us/step - loss: 0.6816 - acc: 0.6694\n",
            "Epoch 7/10\n",
            "3333/3333 [==============================] - 2s 685us/step - loss: 0.6419 - acc: 0.7120\n",
            "Epoch 8/10\n",
            "3333/3333 [==============================] - 2s 687us/step - loss: 0.6108 - acc: 0.7504\n",
            "Epoch 9/10\n",
            "3333/3333 [==============================] - 2s 698us/step - loss: 0.5902 - acc: 0.7711\n",
            "Epoch 10/10\n",
            "3333/3333 [==============================] - 2s 687us/step - loss: 0.5638 - acc: 0.8035\n",
            "1667/1667 [==============================] - 5s 3ms/step\n",
            "Epoch 1/10\n",
            "3334/3334 [==============================] - 15s 4ms/step - loss: 1.0728 - acc: 0.4946\n",
            "Epoch 2/10\n",
            "3334/3334 [==============================] - 2s 692us/step - loss: 1.0071 - acc: 0.4748\n",
            "Epoch 3/10\n",
            "3334/3334 [==============================] - 2s 710us/step - loss: 0.9231 - acc: 0.5180\n",
            "Epoch 4/10\n",
            "3334/3334 [==============================] - 2s 710us/step - loss: 0.8562 - acc: 0.5519\n",
            "Epoch 5/10\n",
            "3334/3334 [==============================] - 2s 712us/step - loss: 0.7991 - acc: 0.5912\n",
            "Epoch 6/10\n",
            "3334/3334 [==============================] - 2s 679us/step - loss: 0.7448 - acc: 0.6302\n",
            "Epoch 7/10\n",
            "3334/3334 [==============================] - 2s 684us/step - loss: 0.7045 - acc: 0.6575\n",
            "Epoch 8/10\n",
            "3334/3334 [==============================] - 2s 691us/step - loss: 0.6643 - acc: 0.7049\n",
            "Epoch 9/10\n",
            "3334/3334 [==============================] - 2s 686us/step - loss: 0.6331 - acc: 0.7229\n",
            "Epoch 10/10\n",
            "3334/3334 [==============================] - 2s 690us/step - loss: 0.5894 - acc: 0.7612\n",
            "1666/1666 [==============================] - 5s 3ms/step\n",
            "Epoch 1/10\n",
            "3333/3333 [==============================] - 14s 4ms/step - loss: 0.9862 - acc: 0.4608\n",
            "Epoch 2/10\n",
            "3333/3333 [==============================] - 1s 342us/step - loss: 0.9398 - acc: 0.4722\n",
            "Epoch 3/10\n",
            "3333/3333 [==============================] - 1s 355us/step - loss: 0.8846 - acc: 0.4956\n",
            "Epoch 4/10\n",
            "3333/3333 [==============================] - 1s 366us/step - loss: 0.8361 - acc: 0.5173\n",
            "Epoch 5/10\n",
            "3333/3333 [==============================] - 1s 355us/step - loss: 0.8045 - acc: 0.5371\n",
            "Epoch 6/10\n",
            "3333/3333 [==============================] - 1s 366us/step - loss: 0.7687 - acc: 0.5644\n",
            "Epoch 7/10\n",
            "3333/3333 [==============================] - 1s 367us/step - loss: 0.7396 - acc: 0.5863\n",
            "Epoch 8/10\n",
            "3333/3333 [==============================] - 1s 352us/step - loss: 0.7153 - acc: 0.6061\n",
            "Epoch 9/10\n",
            "3333/3333 [==============================] - 1s 364us/step - loss: 0.6899 - acc: 0.6184\n",
            "Epoch 10/10\n",
            "3333/3333 [==============================] - 1s 347us/step - loss: 0.6717 - acc: 0.6505\n",
            "1667/1667 [==============================] - 5s 3ms/step\n",
            "Epoch 1/10\n",
            "3333/3333 [==============================] - 15s 4ms/step - loss: 0.8170 - acc: 0.5371\n",
            "Epoch 2/10\n",
            "3333/3333 [==============================] - 1s 340us/step - loss: 0.7825 - acc: 0.5434\n",
            "Epoch 3/10\n",
            "3333/3333 [==============================] - 1s 344us/step - loss: 0.7563 - acc: 0.5692\n",
            "Epoch 4/10\n",
            "3333/3333 [==============================] - 1s 362us/step - loss: 0.7128 - acc: 0.5965\n",
            "Epoch 5/10\n",
            "3333/3333 [==============================] - 1s 356us/step - loss: 0.7001 - acc: 0.6169\n",
            "Epoch 6/10\n",
            "3333/3333 [==============================] - 1s 356us/step - loss: 0.6778 - acc: 0.6364\n",
            "Epoch 7/10\n",
            "3333/3333 [==============================] - 1s 359us/step - loss: 0.6570 - acc: 0.6553\n",
            "Epoch 8/10\n",
            "3333/3333 [==============================] - 1s 358us/step - loss: 0.6401 - acc: 0.6757\n",
            "Epoch 9/10\n",
            "3333/3333 [==============================] - 1s 361us/step - loss: 0.6162 - acc: 0.7009\n",
            "Epoch 10/10\n",
            "3333/3333 [==============================] - 1s 353us/step - loss: 0.6008 - acc: 0.7138\n",
            "1667/1667 [==============================] - 6s 3ms/step\n",
            "Epoch 1/10\n",
            "3334/3334 [==============================] - 15s 5ms/step - loss: 0.8030 - acc: 0.5204\n",
            "Epoch 2/10\n",
            "3334/3334 [==============================] - 1s 354us/step - loss: 0.7693 - acc: 0.5291\n",
            "Epoch 3/10\n",
            "3334/3334 [==============================] - 1s 362us/step - loss: 0.7328 - acc: 0.5558\n",
            "Epoch 4/10\n",
            "3334/3334 [==============================] - 1s 358us/step - loss: 0.7053 - acc: 0.5825\n",
            "Epoch 5/10\n",
            "3334/3334 [==============================] - 1s 349us/step - loss: 0.6655 - acc: 0.6218\n",
            "Epoch 6/10\n",
            "3334/3334 [==============================] - 1s 354us/step - loss: 0.6475 - acc: 0.6269\n",
            "Epoch 7/10\n",
            "3334/3334 [==============================] - 1s 346us/step - loss: 0.6278 - acc: 0.6560\n",
            "Epoch 8/10\n",
            "3334/3334 [==============================] - 1s 352us/step - loss: 0.6032 - acc: 0.6716\n",
            "Epoch 9/10\n",
            "3334/3334 [==============================] - 1s 358us/step - loss: 0.5876 - acc: 0.6866\n",
            "Epoch 10/10\n",
            "3334/3334 [==============================] - 1s 353us/step - loss: 0.5683 - acc: 0.7043\n",
            "1666/1666 [==============================] - 6s 4ms/step\n",
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 18s 4ms/step - loss: 0.7347 - acc: 0.5848\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 3s 660us/step - loss: 0.6795 - acc: 0.6348\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 3s 669us/step - loss: 0.6261 - acc: 0.6928\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 3s 670us/step - loss: 0.5876 - acc: 0.7330\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 3s 672us/step - loss: 0.5580 - acc: 0.7542\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 3s 668us/step - loss: 0.5159 - acc: 0.7876\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 3s 670us/step - loss: 0.4936 - acc: 0.8048\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 3s 674us/step - loss: 0.4585 - acc: 0.8350\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 3s 668us/step - loss: 0.4332 - acc: 0.8508\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 3s 662us/step - loss: 0.4115 - acc: 0.8608\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
              "             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7fe345574128>,\n",
              "             iid='warn', n_jobs=None,\n",
              "             param_grid={'batch_size': [256, 512], 'epochs': [10]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97-rNFxifMqm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c0ea3c5a-c8aa-477f-be62-ba8538a3f0a6"
      },
      "source": [
        "print(clf.best_score_,clf.best_params_)\n",
        "means=clf.cv_results_['mean_test_score']\n",
        "parameters=clf.cv_results_['params']\n",
        "for mean,parameter in zip(means,parameters):\n",
        "  print(mean,parameter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9991999992370606 {'batch_size': 256, 'epochs': 10}\n",
            "0.9991999992370606 {'batch_size': 256, 'epochs': 10}\n",
            "0.9330000005245209 {'batch_size': 512, 'epochs': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBYHm_Z6w1Ir"
      },
      "source": [
        "Tuning optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFbG3obwgTdq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4157594-521c-4e3e-dc23-528a0156b144"
      },
      "source": [
        "def c_model(optimizer):\n",
        "              model1=Sequential()\n",
        "              emb_input=Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
        "              embeddings=Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH, trainable=False)(emb_input)\n",
        "              lstm_layer = Bidirectional(LSTM((32),input_shape=(None,50),return_sequences=False, recurrent_dropout=0.5, dropout=0.5))(embeddings)  \n",
        "              output=Dense(16,kernel_regularizer=regularizers.l2(0.001), activation='elu')(lstm_layer)\n",
        "              output=keras.layers.Dropout(0.4)(output)\n",
        "              output = BatchNormalization()(output)             \n",
        "              output=Dense(8,kernel_regularizer=regularizers.l2(0.001), activation='elu')(output)\n",
        "              output=keras.layers.Dropout(0.4)(output)\n",
        "              output = BatchNormalization()(output)\n",
        "              output=Dense(4,kernel_regularizer=regularizers.l2(0.001), activation='elu')(output)\n",
        "              output=keras.layers.Dropout(0.4)(output)\n",
        "              output=Dense(1,activation='sigmoid')(output)      \n",
        "              model1 = Model(inputs=[emb_input], outputs=[output])\n",
        "              #keras.optimizers.Adam(lr=0.12, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "              model1.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "              return model1\n",
        "model2=KerasClassifier(build_fn=c_model,epochs=10,batch_size=512)   \n",
        "parameters={'optimizer':['SGD','RMSprop','Adagrad','Adam']}\n",
        "clf = GridSearchCV(model2,parameters)\n",
        "clf.fit(train_data,train_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "3333/3333 [==============================] - 16s 5ms/step - loss: 0.9110 - acc: 0.5254\n",
            "Epoch 2/10\n",
            "3333/3333 [==============================] - 1s 345us/step - loss: 0.8647 - acc: 0.5539\n",
            "Epoch 3/10\n",
            "3333/3333 [==============================] - 1s 344us/step - loss: 0.8282 - acc: 0.5674\n",
            "Epoch 4/10\n",
            "3333/3333 [==============================] - 1s 340us/step - loss: 0.7863 - acc: 0.5845\n",
            "Epoch 5/10\n",
            "3333/3333 [==============================] - 1s 364us/step - loss: 0.7664 - acc: 0.5827\n",
            "Epoch 6/10\n",
            "3333/3333 [==============================] - 1s 361us/step - loss: 0.7320 - acc: 0.6214\n",
            "Epoch 7/10\n",
            "3333/3333 [==============================] - 1s 352us/step - loss: 0.7046 - acc: 0.6313\n",
            "Epoch 8/10\n",
            "3333/3333 [==============================] - 1s 339us/step - loss: 0.6875 - acc: 0.6541\n",
            "Epoch 9/10\n",
            "3333/3333 [==============================] - 1s 352us/step - loss: 0.6630 - acc: 0.6739\n",
            "Epoch 10/10\n",
            "3333/3333 [==============================] - 1s 353us/step - loss: 0.6447 - acc: 0.6889\n",
            "1667/1667 [==============================] - 6s 4ms/step\n",
            "Epoch 1/10\n",
            "3333/3333 [==============================] - 17s 5ms/step - loss: 0.7563 - acc: 0.5584\n",
            "Epoch 2/10\n",
            "3333/3333 [==============================] - 1s 359us/step - loss: 0.7277 - acc: 0.5830\n",
            "Epoch 3/10\n",
            "3333/3333 [==============================] - 1s 345us/step - loss: 0.7073 - acc: 0.6172\n",
            "Epoch 4/10\n",
            "3333/3333 [==============================] - 1s 336us/step - loss: 0.6724 - acc: 0.6412\n",
            "Epoch 5/10\n",
            "3333/3333 [==============================] - 1s 347us/step - loss: 0.6573 - acc: 0.6547\n",
            "Epoch 6/10\n",
            "3333/3333 [==============================] - 1s 363us/step - loss: 0.6369 - acc: 0.6781\n",
            "Epoch 7/10\n",
            "3333/3333 [==============================] - 1s 338us/step - loss: 0.6215 - acc: 0.6862\n",
            "Epoch 8/10\n",
            "3333/3333 [==============================] - 1s 350us/step - loss: 0.6013 - acc: 0.7138\n",
            "Epoch 9/10\n",
            "3333/3333 [==============================] - 1s 351us/step - loss: 0.5859 - acc: 0.7390\n",
            "Epoch 10/10\n",
            "3333/3333 [==============================] - 1s 365us/step - loss: 0.5729 - acc: 0.7489\n",
            "1667/1667 [==============================] - 7s 4ms/step\n",
            "Epoch 1/10\n",
            "3334/3334 [==============================] - 17s 5ms/step - loss: 1.3032 - acc: 0.4886\n",
            "Epoch 2/10\n",
            "3334/3334 [==============================] - 1s 351us/step - loss: 1.1451 - acc: 0.5084\n",
            "Epoch 3/10\n",
            "3334/3334 [==============================] - 1s 363us/step - loss: 1.0762 - acc: 0.5150\n",
            "Epoch 4/10\n",
            "3334/3334 [==============================] - 1s 360us/step - loss: 1.0175 - acc: 0.5432\n",
            "Epoch 5/10\n",
            "3334/3334 [==============================] - 1s 360us/step - loss: 0.9661 - acc: 0.5534\n",
            "Epoch 6/10\n",
            "3334/3334 [==============================] - 1s 368us/step - loss: 0.9136 - acc: 0.5702\n",
            "Epoch 7/10\n",
            "3334/3334 [==============================] - 1s 342us/step - loss: 0.8725 - acc: 0.5807\n",
            "Epoch 8/10\n",
            "3334/3334 [==============================] - 1s 359us/step - loss: 0.8356 - acc: 0.6095\n",
            "Epoch 9/10\n",
            "3334/3334 [==============================] - 1s 368us/step - loss: 0.7949 - acc: 0.6278\n",
            "Epoch 10/10\n",
            "3334/3334 [==============================] - 1s 348us/step - loss: 0.7628 - acc: 0.6497\n",
            "1666/1666 [==============================] - 7s 4ms/step\n",
            "Epoch 1/10\n",
            "3333/3333 [==============================] - 18s 5ms/step - loss: 0.9417 - acc: 0.5179\n",
            "Epoch 2/10\n",
            "3333/3333 [==============================] - 1s 350us/step - loss: 0.8785 - acc: 0.5335\n",
            "Epoch 3/10\n",
            "3333/3333 [==============================] - 1s 360us/step - loss: 0.8291 - acc: 0.5602\n",
            "Epoch 4/10\n",
            "3333/3333 [==============================] - 1s 351us/step - loss: 0.7966 - acc: 0.5893\n",
            "Epoch 5/10\n",
            "3333/3333 [==============================] - 1s 352us/step - loss: 0.7649 - acc: 0.5917\n",
            "Epoch 6/10\n",
            "3333/3333 [==============================] - 1s 363us/step - loss: 0.7252 - acc: 0.6112\n",
            "Epoch 7/10\n",
            "3333/3333 [==============================] - 1s 357us/step - loss: 0.6910 - acc: 0.6565\n",
            "Epoch 8/10\n",
            "3333/3333 [==============================] - 1s 359us/step - loss: 0.6506 - acc: 0.6667\n",
            "Epoch 9/10\n",
            "3333/3333 [==============================] - 1s 357us/step - loss: 0.6384 - acc: 0.6811\n",
            "Epoch 10/10\n",
            "3333/3333 [==============================] - 1s 346us/step - loss: 0.6045 - acc: 0.7060\n",
            "1667/1667 [==============================] - 7s 4ms/step\n",
            "Epoch 1/10\n",
            "3333/3333 [==============================] - 18s 5ms/step - loss: 0.9148 - acc: 0.4875\n",
            "Epoch 2/10\n",
            "3333/3333 [==============================] - 1s 336us/step - loss: 0.8753 - acc: 0.5050\n",
            "Epoch 3/10\n",
            "3333/3333 [==============================] - 1s 343us/step - loss: 0.8387 - acc: 0.5149\n",
            "Epoch 4/10\n",
            "3333/3333 [==============================] - 1s 340us/step - loss: 0.7839 - acc: 0.5647\n",
            "Epoch 5/10\n",
            "3333/3333 [==============================] - 1s 344us/step - loss: 0.7836 - acc: 0.5737\n",
            "Epoch 6/10\n",
            "3333/3333 [==============================] - 1s 367us/step - loss: 0.7434 - acc: 0.5869\n",
            "Epoch 7/10\n",
            "3333/3333 [==============================] - 1s 357us/step - loss: 0.7288 - acc: 0.6040\n",
            "Epoch 8/10\n",
            "3333/3333 [==============================] - 1s 363us/step - loss: 0.7017 - acc: 0.6268\n",
            "Epoch 9/10\n",
            "3333/3333 [==============================] - 1s 347us/step - loss: 0.6904 - acc: 0.6418\n",
            "Epoch 10/10\n",
            "3333/3333 [==============================] - 1s 337us/step - loss: 0.6697 - acc: 0.6604\n",
            "1667/1667 [==============================] - 7s 4ms/step\n",
            "Epoch 1/10\n",
            "3334/3334 [==============================] - 19s 6ms/step - loss: 0.9921 - acc: 0.4730\n",
            "Epoch 2/10\n",
            "3334/3334 [==============================] - 1s 351us/step - loss: 0.9433 - acc: 0.5039\n",
            "Epoch 3/10\n",
            "3334/3334 [==============================] - 1s 350us/step - loss: 0.8916 - acc: 0.5132\n",
            "Epoch 4/10\n",
            "3334/3334 [==============================] - 1s 334us/step - loss: 0.8514 - acc: 0.5393\n",
            "Epoch 5/10\n",
            "3334/3334 [==============================] - 1s 352us/step - loss: 0.8238 - acc: 0.5483\n",
            "Epoch 6/10\n",
            "3334/3334 [==============================] - 1s 341us/step - loss: 0.7985 - acc: 0.5864\n",
            "Epoch 7/10\n",
            "3334/3334 [==============================] - 1s 361us/step - loss: 0.7785 - acc: 0.6047\n",
            "Epoch 8/10\n",
            "3334/3334 [==============================] - 1s 373us/step - loss: 0.7571 - acc: 0.6233\n",
            "Epoch 9/10\n",
            "3334/3334 [==============================] - 1s 356us/step - loss: 0.7377 - acc: 0.6536\n",
            "Epoch 10/10\n",
            "3334/3334 [==============================] - 1s 344us/step - loss: 0.7185 - acc: 0.6620\n",
            "1666/1666 [==============================] - 7s 4ms/step\n",
            "Epoch 1/10\n",
            "3333/3333 [==============================] - 19s 6ms/step - loss: 0.9416 - acc: 0.5083\n",
            "Epoch 2/10\n",
            "3333/3333 [==============================] - 1s 355us/step - loss: 0.8744 - acc: 0.5329\n",
            "Epoch 3/10\n",
            "3333/3333 [==============================] - 1s 351us/step - loss: 0.8207 - acc: 0.5668\n",
            "Epoch 4/10\n",
            "3333/3333 [==============================] - 1s 355us/step - loss: 0.7727 - acc: 0.5860\n",
            "Epoch 5/10\n",
            "3333/3333 [==============================] - 1s 347us/step - loss: 0.7377 - acc: 0.6052\n",
            "Epoch 6/10\n",
            "3333/3333 [==============================] - 1s 352us/step - loss: 0.6848 - acc: 0.6424\n",
            "Epoch 7/10\n",
            "3333/3333 [==============================] - 1s 357us/step - loss: 0.6884 - acc: 0.6427\n",
            "Epoch 8/10\n",
            "3333/3333 [==============================] - 1s 360us/step - loss: 0.6621 - acc: 0.6418\n",
            "Epoch 9/10\n",
            "3333/3333 [==============================] - 1s 351us/step - loss: 0.6200 - acc: 0.6916\n",
            "Epoch 10/10\n",
            "3333/3333 [==============================] - 1s 338us/step - loss: 0.6102 - acc: 0.6847\n",
            "1667/1667 [==============================] - 8s 5ms/step\n",
            "Epoch 1/10\n",
            "3333/3333 [==============================] - 20s 6ms/step - loss: 0.9737 - acc: 0.4101\n",
            "Epoch 2/10\n",
            "3333/3333 [==============================] - 1s 352us/step - loss: 0.9307 - acc: 0.4347\n",
            "Epoch 3/10\n",
            "3333/3333 [==============================] - 1s 361us/step - loss: 0.9067 - acc: 0.4443\n",
            "Epoch 4/10\n",
            "3333/3333 [==============================] - 1s 364us/step - loss: 0.8848 - acc: 0.4518\n",
            "Epoch 5/10\n",
            "3333/3333 [==============================] - 1s 358us/step - loss: 0.8517 - acc: 0.4854\n",
            "Epoch 6/10\n",
            "3333/3333 [==============================] - 1s 360us/step - loss: 0.8383 - acc: 0.4851\n",
            "Epoch 7/10\n",
            "3333/3333 [==============================] - 1s 362us/step - loss: 0.8043 - acc: 0.5056\n",
            "Epoch 8/10\n",
            "3333/3333 [==============================] - 1s 341us/step - loss: 0.7850 - acc: 0.5200\n",
            "Epoch 9/10\n",
            "3333/3333 [==============================] - 1s 369us/step - loss: 0.7677 - acc: 0.5440\n",
            "Epoch 10/10\n",
            "3333/3333 [==============================] - 1s 356us/step - loss: 0.7375 - acc: 0.5845\n",
            "1667/1667 [==============================] - 8s 5ms/step\n",
            "Epoch 1/10\n",
            "3334/3334 [==============================] - 20s 6ms/step - loss: 0.8409 - acc: 0.5081\n",
            "Epoch 2/10\n",
            "3334/3334 [==============================] - 1s 336us/step - loss: 0.7878 - acc: 0.5414\n",
            "Epoch 3/10\n",
            "3334/3334 [==============================] - 1s 343us/step - loss: 0.7322 - acc: 0.5690\n",
            "Epoch 4/10\n",
            "3334/3334 [==============================] - 1s 350us/step - loss: 0.7167 - acc: 0.5927\n",
            "Epoch 5/10\n",
            "3334/3334 [==============================] - 1s 349us/step - loss: 0.6870 - acc: 0.6032\n",
            "Epoch 6/10\n",
            "3334/3334 [==============================] - 1s 332us/step - loss: 0.6607 - acc: 0.6302\n",
            "Epoch 7/10\n",
            "3334/3334 [==============================] - 1s 355us/step - loss: 0.6370 - acc: 0.6464\n",
            "Epoch 8/10\n",
            "3334/3334 [==============================] - 1s 358us/step - loss: 0.6159 - acc: 0.6647\n",
            "Epoch 9/10\n",
            "3334/3334 [==============================] - 1s 352us/step - loss: 0.6029 - acc: 0.6737\n",
            "Epoch 10/10\n",
            "3334/3334 [==============================] - 1s 347us/step - loss: 0.5967 - acc: 0.6779\n",
            "1666/1666 [==============================] - 8s 5ms/step\n",
            "Epoch 1/10\n",
            "3333/3333 [==============================] - 21s 6ms/step - loss: 0.9275 - acc: 0.5002\n",
            "Epoch 2/10\n",
            "3333/3333 [==============================] - 1s 348us/step - loss: 0.8725 - acc: 0.5329\n",
            "Epoch 3/10\n",
            "3333/3333 [==============================] - 1s 346us/step - loss: 0.8422 - acc: 0.5335\n",
            "Epoch 4/10\n",
            "3333/3333 [==============================] - 1s 355us/step - loss: 0.8014 - acc: 0.5587\n",
            "Epoch 5/10\n",
            "3333/3333 [==============================] - 1s 347us/step - loss: 0.7776 - acc: 0.5701\n",
            "Epoch 6/10\n",
            "3333/3333 [==============================] - 1s 358us/step - loss: 0.7422 - acc: 0.5860\n",
            "Epoch 7/10\n",
            "3333/3333 [==============================] - 1s 351us/step - loss: 0.7189 - acc: 0.5974\n",
            "Epoch 8/10\n",
            "3333/3333 [==============================] - 1s 344us/step - loss: 0.6982 - acc: 0.6208\n",
            "Epoch 9/10\n",
            "3333/3333 [==============================] - 1s 352us/step - loss: 0.6688 - acc: 0.6334\n",
            "Epoch 10/10\n",
            "3333/3333 [==============================] - 1s 356us/step - loss: 0.6491 - acc: 0.6505\n",
            "1667/1667 [==============================] - 8s 5ms/step\n",
            "Epoch 1/10\n",
            "3333/3333 [==============================] - 21s 6ms/step - loss: 0.9691 - acc: 0.5221\n",
            "Epoch 2/10\n",
            "3333/3333 [==============================] - 1s 356us/step - loss: 0.9399 - acc: 0.5491\n",
            "Epoch 3/10\n",
            "3333/3333 [==============================] - 1s 351us/step - loss: 0.8903 - acc: 0.5635\n",
            "Epoch 4/10\n",
            "3333/3333 [==============================] - 1s 355us/step - loss: 0.8601 - acc: 0.5719\n",
            "Epoch 5/10\n",
            "3333/3333 [==============================] - 1s 342us/step - loss: 0.8333 - acc: 0.5947\n",
            "Epoch 6/10\n",
            "3333/3333 [==============================] - 1s 365us/step - loss: 0.8004 - acc: 0.6082\n",
            "Epoch 7/10\n",
            "3333/3333 [==============================] - 1s 340us/step - loss: 0.7871 - acc: 0.6373\n",
            "Epoch 8/10\n",
            "3333/3333 [==============================] - 1s 363us/step - loss: 0.7551 - acc: 0.6550\n",
            "Epoch 9/10\n",
            "3333/3333 [==============================] - 1s 357us/step - loss: 0.7309 - acc: 0.6823\n",
            "Epoch 10/10\n",
            "3333/3333 [==============================] - 1s 354us/step - loss: 0.7146 - acc: 0.6907\n",
            "1667/1667 [==============================] - 9s 5ms/step\n",
            "Epoch 1/10\n",
            "3334/3334 [==============================] - 22s 6ms/step - loss: 0.8107 - acc: 0.5813\n",
            "Epoch 2/10\n",
            "3334/3334 [==============================] - 1s 364us/step - loss: 0.7830 - acc: 0.6029\n",
            "Epoch 3/10\n",
            "3334/3334 [==============================] - 1s 333us/step - loss: 0.7521 - acc: 0.6104\n",
            "Epoch 4/10\n",
            "3334/3334 [==============================] - 1s 341us/step - loss: 0.7188 - acc: 0.6425\n",
            "Epoch 5/10\n",
            "3334/3334 [==============================] - 1s 355us/step - loss: 0.7029 - acc: 0.6536\n",
            "Epoch 6/10\n",
            "3334/3334 [==============================] - 1s 345us/step - loss: 0.6743 - acc: 0.6797\n",
            "Epoch 7/10\n",
            "3334/3334 [==============================] - 1s 358us/step - loss: 0.6593 - acc: 0.6764\n",
            "Epoch 8/10\n",
            "3334/3334 [==============================] - 1s 343us/step - loss: 0.6420 - acc: 0.7088\n",
            "Epoch 9/10\n",
            "3334/3334 [==============================] - 1s 356us/step - loss: 0.6309 - acc: 0.7061\n",
            "Epoch 10/10\n",
            "3334/3334 [==============================] - 1s 348us/step - loss: 0.6078 - acc: 0.7232\n",
            "1666/1666 [==============================] - 9s 5ms/step\n",
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 23s 5ms/step - loss: 0.7259 - acc: 0.5368\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 2s 340us/step - loss: 0.6948 - acc: 0.5742\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 2s 332us/step - loss: 0.6663 - acc: 0.5910\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 2s 341us/step - loss: 0.6301 - acc: 0.6482\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 2s 348us/step - loss: 0.6119 - acc: 0.6720\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 2s 321us/step - loss: 0.5816 - acc: 0.7012\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 2s 346us/step - loss: 0.5588 - acc: 0.7276\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 2s 333us/step - loss: 0.5355 - acc: 0.7502\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 2s 342us/step - loss: 0.5126 - acc: 0.7758\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 2s 339us/step - loss: 0.4839 - acc: 0.8052\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
              "             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7fe332a0f048>,\n",
              "             iid='warn', n_jobs=None,\n",
              "             param_grid={'optimizer': ['SGD', 'RMSprop', 'Adagrad', 'Adam']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5_RK5l0iz16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "46d94d61-0aa2-42a5-cb4a-6a5b088904d0"
      },
      "source": [
        "print(clf.best_score_,clf.best_params_)\n",
        "means=clf.cv_results_['mean_test_score']\n",
        "parameters=clf.cv_results_['params']\n",
        "for mean,parameter in zip(means,parameters):\n",
        "  print(mean,parameter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9852000007987023 {'optimizer': 'RMSprop'}\n",
            "0.9013999992251396 {'optimizer': 'SGD'}\n",
            "0.9852000007987023 {'optimizer': 'RMSprop'}\n",
            "0.9523999998450279 {'optimizer': 'Adagrad'}\n",
            "0.9103999995589256 {'optimizer': 'Adam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL3BpV_9w-Ic"
      },
      "source": [
        "Tuning Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9sWle9ajIy-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9322a58-17fb-4982-ba2f-04ce811bd40e"
      },
      "source": [
        "def c_model(activation):\n",
        "              model1=Sequential()\n",
        "              emb_input=Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
        "              embeddings=Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH, trainable=False)(emb_input)\n",
        "              lstm_layer = Bidirectional(LSTM((32),input_shape=(None,50),return_sequences=False, recurrent_dropout=0.5, dropout=0.5))(embeddings)  \n",
        "              output=Dense(16,kernel_regularizer=regularizers.l2(0.001), activation='elu')(lstm_layer)\n",
        "              output=keras.layers.Dropout(0.4)(output)\n",
        "              output = BatchNormalization()(output)             \n",
        "              output=Dense(8,kernel_regularizer=regularizers.l2(0.001), activation='elu')(output)\n",
        "              output=keras.layers.Dropout(0.4)(output)\n",
        "              output = BatchNormalization()(output)\n",
        "              output=Dense(4,kernel_regularizer=regularizers.l2(0.001), activation='elu')(output)\n",
        "              output=keras.layers.Dropout(0.4)(output)\n",
        "              output=Dense(1,activation='sigmoid')(output)      \n",
        "              model1 = Model(inputs=[emb_input], outputs=[output])\n",
        "              #keras.optimizers.Adam(lr=0.12, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "              model1.compile(optimizer='Adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "              return model1\n",
        "model2=KerasClassifier(build_fn=c_model,epochs=10,batch_size=512) \n",
        "parameters={'activation':['softmax','relu','tanh','sigmoid','linear']}\n",
        "clf = GridSearchCV(model2,parameters)\n",
        "clf.fit(train_data,train_label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "13333/13333 [==============================] - 23s 2ms/step - loss: 0.6143 - acc: 0.9973\n",
            "Epoch 2/10\n",
            "13333/13333 [==============================] - 4s 337us/step - loss: 0.5094 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "13333/13333 [==============================] - 5s 342us/step - loss: 0.4522 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "13333/13333 [==============================] - 5s 340us/step - loss: 0.4175 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "13333/13333 [==============================] - 4s 335us/step - loss: 0.3928 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "13333/13333 [==============================] - 4s 335us/step - loss: 0.3711 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "13333/13333 [==============================] - 5s 346us/step - loss: 0.3549 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13333/13333 [==============================] - 5s 347us/step - loss: 0.3357 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13333/13333 [==============================] - 4s 337us/step - loss: 0.3200 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13333/13333 [==============================] - 4s 331us/step - loss: 0.3062 - acc: 1.0000\n",
            "6667/6667 [==============================] - 8s 1ms/step\n",
            "Epoch 1/10\n",
            "13333/13333 [==============================] - 23s 2ms/step - loss: 0.6358 - acc: 0.7995\n",
            "Epoch 2/10\n",
            "13333/13333 [==============================] - 5s 341us/step - loss: 0.5131 - acc: 0.9994\n",
            "Epoch 3/10\n",
            "13333/13333 [==============================] - 5s 347us/step - loss: 0.4351 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "13333/13333 [==============================] - 5s 348us/step - loss: 0.3934 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "13333/13333 [==============================] - 5s 340us/step - loss: 0.3633 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "13333/13333 [==============================] - 5s 339us/step - loss: 0.3416 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "13333/13333 [==============================] - 4s 335us/step - loss: 0.3235 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13333/13333 [==============================] - 5s 341us/step - loss: 0.3073 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13333/13333 [==============================] - 5s 342us/step - loss: 0.2937 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13333/13333 [==============================] - 4s 337us/step - loss: 0.2802 - acc: 1.0000\n",
            "6667/6667 [==============================] - 9s 1ms/step\n",
            "Epoch 1/10\n",
            "13334/13334 [==============================] - 24s 2ms/step - loss: 0.5673 - acc: 0.9841\n",
            "Epoch 2/10\n",
            "13334/13334 [==============================] - 4s 337us/step - loss: 0.4167 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "13334/13334 [==============================] - 5s 345us/step - loss: 0.3684 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "13334/13334 [==============================] - 5s 345us/step - loss: 0.3436 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "13334/13334 [==============================] - 5s 345us/step - loss: 0.3252 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "13334/13334 [==============================] - 4s 332us/step - loss: 0.3093 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "13334/13334 [==============================] - 4s 335us/step - loss: 0.2952 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13334/13334 [==============================] - 5s 341us/step - loss: 0.2821 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13334/13334 [==============================] - 4s 336us/step - loss: 0.2702 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13334/13334 [==============================] - 4s 337us/step - loss: 0.2590 - acc: 1.0000\n",
            "6666/6666 [==============================] - 9s 1ms/step\n",
            "Epoch 1/10\n",
            "13333/13333 [==============================] - 24s 2ms/step - loss: 0.4564 - acc: 0.8217\n",
            "Epoch 2/10\n",
            "13333/13333 [==============================] - 5s 338us/step - loss: 0.1962 - acc: 0.9983\n",
            "Epoch 3/10\n",
            "13333/13333 [==============================] - 5s 341us/step - loss: 0.1102 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "13333/13333 [==============================] - 4s 332us/step - loss: 0.0665 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "13333/13333 [==============================] - 5s 346us/step - loss: 0.0413 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "13333/13333 [==============================] - 4s 333us/step - loss: 0.0270 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "13333/13333 [==============================] - 4s 337us/step - loss: 0.0184 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13333/13333 [==============================] - 5s 346us/step - loss: 0.0133 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13333/13333 [==============================] - 4s 336us/step - loss: 0.0099 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13333/13333 [==============================] - 4s 330us/step - loss: 0.0077 - acc: 1.0000\n",
            "6667/6667 [==============================] - 9s 1ms/step\n",
            "Epoch 1/10\n",
            "13333/13333 [==============================] - 24s 2ms/step - loss: 0.5248 - acc: 0.7745\n",
            "Epoch 2/10\n",
            "13333/13333 [==============================] - 5s 340us/step - loss: 0.2985 - acc: 0.9948\n",
            "Epoch 3/10\n",
            "13333/13333 [==============================] - 4s 334us/step - loss: 0.1952 - acc: 0.9998\n",
            "Epoch 4/10\n",
            "13333/13333 [==============================] - 5s 343us/step - loss: 0.1295 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "13333/13333 [==============================] - 5s 343us/step - loss: 0.0842 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "13333/13333 [==============================] - 4s 334us/step - loss: 0.0555 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "13333/13333 [==============================] - 4s 330us/step - loss: 0.0383 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13333/13333 [==============================] - 5s 340us/step - loss: 0.0280 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13333/13333 [==============================] - 4s 330us/step - loss: 0.0210 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13333/13333 [==============================] - 4s 333us/step - loss: 0.0164 - acc: 1.0000\n",
            "6667/6667 [==============================] - 9s 1ms/step\n",
            "Epoch 1/10\n",
            "13334/13334 [==============================] - 25s 2ms/step - loss: 0.6150 - acc: 0.6179\n",
            "Epoch 2/10\n",
            "13334/13334 [==============================] - 5s 338us/step - loss: 0.3138 - acc: 0.9530\n",
            "Epoch 3/10\n",
            "13334/13334 [==============================] - 5s 338us/step - loss: 0.2105 - acc: 0.9975\n",
            "Epoch 4/10\n",
            "13334/13334 [==============================] - 4s 337us/step - loss: 0.1243 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "13334/13334 [==============================] - 5s 341us/step - loss: 0.0697 - acc: 0.9999\n",
            "Epoch 6/10\n",
            "13334/13334 [==============================] - 5s 340us/step - loss: 0.0430 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "13334/13334 [==============================] - 5s 345us/step - loss: 0.0315 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13334/13334 [==============================] - 5s 338us/step - loss: 0.0216 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13334/13334 [==============================] - 4s 331us/step - loss: 0.0161 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13334/13334 [==============================] - 4s 336us/step - loss: 0.0125 - acc: 1.0000\n",
            "6666/6666 [==============================] - 9s 1ms/step\n",
            "Epoch 1/10\n",
            "13333/13333 [==============================] - 25s 2ms/step - loss: 0.6139 - acc: 0.7433\n",
            "Epoch 2/10\n",
            "13333/13333 [==============================] - 4s 328us/step - loss: 0.4145 - acc: 0.9870\n",
            "Epoch 3/10\n",
            "13333/13333 [==============================] - 5s 341us/step - loss: 0.2990 - acc: 0.9988\n",
            "Epoch 4/10\n",
            "13333/13333 [==============================] - 4s 328us/step - loss: 0.2229 - acc: 0.9998\n",
            "Epoch 5/10\n",
            "13333/13333 [==============================] - 4s 334us/step - loss: 0.1713 - acc: 0.9999\n",
            "Epoch 6/10\n",
            "13333/13333 [==============================] - 4s 333us/step - loss: 0.1365 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "13333/13333 [==============================] - 5s 346us/step - loss: 0.1123 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13333/13333 [==============================] - 5s 345us/step - loss: 0.0934 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13333/13333 [==============================] - 5s 339us/step - loss: 0.0777 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13333/13333 [==============================] - 5s 338us/step - loss: 0.0652 - acc: 1.0000\n",
            "6667/6667 [==============================] - 10s 1ms/step\n",
            "Epoch 1/10\n",
            "13333/13333 [==============================] - 26s 2ms/step - loss: 0.6245 - acc: 0.7069\n",
            "Epoch 2/10\n",
            "13333/13333 [==============================] - 5s 340us/step - loss: 0.4167 - acc: 0.9687\n",
            "Epoch 3/10\n",
            "13333/13333 [==============================] - 5s 341us/step - loss: 0.3007 - acc: 0.9952\n",
            "Epoch 4/10\n",
            "13333/13333 [==============================] - 5s 350us/step - loss: 0.2238 - acc: 0.9982\n",
            "Epoch 5/10\n",
            "13333/13333 [==============================] - 4s 336us/step - loss: 0.1716 - acc: 0.9995\n",
            "Epoch 6/10\n",
            "13333/13333 [==============================] - 5s 338us/step - loss: 0.1341 - acc: 0.9995\n",
            "Epoch 7/10\n",
            "13333/13333 [==============================] - 5s 339us/step - loss: 0.1040 - acc: 0.9998\n",
            "Epoch 8/10\n",
            "13333/13333 [==============================] - 5s 338us/step - loss: 0.0836 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13333/13333 [==============================] - 4s 336us/step - loss: 0.0699 - acc: 0.9998\n",
            "Epoch 10/10\n",
            "13333/13333 [==============================] - 4s 336us/step - loss: 0.0573 - acc: 0.9999\n",
            "6667/6667 [==============================] - 10s 1ms/step\n",
            "Epoch 1/10\n",
            "13334/13334 [==============================] - 26s 2ms/step - loss: 0.5991 - acc: 0.7602\n",
            "Epoch 2/10\n",
            "13334/13334 [==============================] - 4s 334us/step - loss: 0.4153 - acc: 0.9666\n",
            "Epoch 3/10\n",
            "13334/13334 [==============================] - 4s 325us/step - loss: 0.3033 - acc: 0.9963\n",
            "Epoch 4/10\n",
            "13334/13334 [==============================] - 5s 339us/step - loss: 0.2275 - acc: 0.9988\n",
            "Epoch 5/10\n",
            "13334/13334 [==============================] - 4s 325us/step - loss: 0.1769 - acc: 0.9994\n",
            "Epoch 6/10\n",
            "13334/13334 [==============================] - 4s 333us/step - loss: 0.1396 - acc: 0.9997\n",
            "Epoch 7/10\n",
            "13334/13334 [==============================] - 5s 340us/step - loss: 0.1124 - acc: 0.9999\n",
            "Epoch 8/10\n",
            "13334/13334 [==============================] - 4s 332us/step - loss: 0.0943 - acc: 0.9997\n",
            "Epoch 9/10\n",
            "13334/13334 [==============================] - 4s 337us/step - loss: 0.0833 - acc: 0.9997\n",
            "Epoch 10/10\n",
            "13334/13334 [==============================] - 4s 335us/step - loss: 0.0716 - acc: 0.9998\n",
            "6666/6666 [==============================] - 10s 1ms/step\n",
            "Epoch 1/10\n",
            "13333/13333 [==============================] - 27s 2ms/step - loss: 0.5167 - acc: 0.9639\n",
            "Epoch 2/10\n",
            "13333/13333 [==============================] - 5s 341us/step - loss: 0.4033 - acc: 0.9990\n",
            "Epoch 3/10\n",
            "13333/13333 [==============================] - 4s 335us/step - loss: 0.3241 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "13333/13333 [==============================] - 5s 338us/step - loss: 0.2683 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "13333/13333 [==============================] - 4s 327us/step - loss: 0.2233 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "13333/13333 [==============================] - 4s 322us/step - loss: 0.1858 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "13333/13333 [==============================] - 4s 334us/step - loss: 0.1554 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13333/13333 [==============================] - 4s 322us/step - loss: 0.1317 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13333/13333 [==============================] - 4s 331us/step - loss: 0.1129 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13333/13333 [==============================] - 5s 340us/step - loss: 0.0978 - acc: 1.0000\n",
            "6667/6667 [==============================] - 10s 2ms/step\n",
            "Epoch 1/10\n",
            "13333/13333 [==============================] - 27s 2ms/step - loss: 0.6997 - acc: 0.5164\n",
            "Epoch 2/10\n",
            "13333/13333 [==============================] - 4s 336us/step - loss: 0.5845 - acc: 0.9477\n",
            "Epoch 3/10\n",
            "13333/13333 [==============================] - 4s 335us/step - loss: 0.4841 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "13333/13333 [==============================] - 4s 334us/step - loss: 0.4006 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "13333/13333 [==============================] - 4s 334us/step - loss: 0.3339 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "13333/13333 [==============================] - 5s 338us/step - loss: 0.2784 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "13333/13333 [==============================] - 4s 336us/step - loss: 0.2330 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13333/13333 [==============================] - 4s 334us/step - loss: 0.1976 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13333/13333 [==============================] - 4s 334us/step - loss: 0.1682 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13333/13333 [==============================] - 4s 332us/step - loss: 0.1458 - acc: 1.0000\n",
            "6667/6667 [==============================] - 10s 2ms/step\n",
            "Epoch 1/10\n",
            "13334/13334 [==============================] - 28s 2ms/step - loss: 0.9623 - acc: 0.0043\n",
            "Epoch 2/10\n",
            "13334/13334 [==============================] - 5s 340us/step - loss: 0.7635 - acc: 0.3201\n",
            "Epoch 3/10\n",
            "13334/13334 [==============================] - 5s 339us/step - loss: 0.6424 - acc: 0.6049\n",
            "Epoch 4/10\n",
            "13334/13334 [==============================] - 4s 334us/step - loss: 0.5517 - acc: 0.7988\n",
            "Epoch 5/10\n",
            "13334/13334 [==============================] - 5s 341us/step - loss: 0.4811 - acc: 0.9902\n",
            "Epoch 6/10\n",
            "13334/13334 [==============================] - 4s 337us/step - loss: 0.4235 - acc: 0.9999\n",
            "Epoch 7/10\n",
            "13334/13334 [==============================] - 4s 333us/step - loss: 0.3769 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13334/13334 [==============================] - 5s 338us/step - loss: 0.3368 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13334/13334 [==============================] - 4s 324us/step - loss: 0.3046 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13334/13334 [==============================] - 4s 332us/step - loss: 0.2793 - acc: 1.0000\n",
            "6666/6666 [==============================] - 11s 2ms/step\n",
            "Epoch 1/10\n",
            "13333/13333 [==============================] - 28s 2ms/step - loss: 0.6875 - acc: 0.6803\n",
            "Epoch 2/10\n",
            "13333/13333 [==============================] - 4s 334us/step - loss: 0.4816 - acc: 0.9932\n",
            "Epoch 3/10\n",
            "13333/13333 [==============================] - 4s 336us/step - loss: 0.3461 - acc: 0.9999\n",
            "Epoch 4/10\n",
            "13333/13333 [==============================] - 5s 338us/step - loss: 0.2253 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "13333/13333 [==============================] - 4s 330us/step - loss: 0.1364 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "13333/13333 [==============================] - 4s 332us/step - loss: 0.0815 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "13333/13333 [==============================] - 4s 334us/step - loss: 0.0511 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13333/13333 [==============================] - 4s 334us/step - loss: 0.0341 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13333/13333 [==============================] - 5s 338us/step - loss: 0.0241 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13333/13333 [==============================] - 5s 338us/step - loss: 0.0178 - acc: 1.0000\n",
            "6667/6667 [==============================] - 11s 2ms/step\n",
            "Epoch 1/10\n",
            "13333/13333 [==============================] - 29s 2ms/step - loss: 0.6671 - acc: 0.7153\n",
            "Epoch 2/10\n",
            "13333/13333 [==============================] - 4s 332us/step - loss: 0.4685 - acc: 0.9947\n",
            "Epoch 3/10\n",
            "13333/13333 [==============================] - 4s 337us/step - loss: 0.3282 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "13333/13333 [==============================] - 4s 337us/step - loss: 0.2088 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "13333/13333 [==============================] - 5s 343us/step - loss: 0.1243 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "13333/13333 [==============================] - 5s 341us/step - loss: 0.0745 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "13333/13333 [==============================] - 5s 338us/step - loss: 0.0468 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13333/13333 [==============================] - 4s 336us/step - loss: 0.0315 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13333/13333 [==============================] - 5s 338us/step - loss: 0.0222 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13333/13333 [==============================] - 4s 333us/step - loss: 0.0165 - acc: 1.0000\n",
            "6667/6667 [==============================] - 11s 2ms/step\n",
            "Epoch 1/10\n",
            "13334/13334 [==============================] - 29s 2ms/step - loss: 0.6783 - acc: 0.6606\n",
            "Epoch 2/10\n",
            "13334/13334 [==============================] - 5s 338us/step - loss: 0.5612 - acc: 0.9831\n",
            "Epoch 3/10\n",
            "13334/13334 [==============================] - 4s 333us/step - loss: 0.4516 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "13334/13334 [==============================] - 5s 338us/step - loss: 0.3226 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "13334/13334 [==============================] - 4s 333us/step - loss: 0.2009 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "13334/13334 [==============================] - 5s 340us/step - loss: 0.1145 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "13334/13334 [==============================] - 4s 329us/step - loss: 0.0655 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "13334/13334 [==============================] - 5s 339us/step - loss: 0.0401 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "13334/13334 [==============================] - 5s 345us/step - loss: 0.0265 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "13334/13334 [==============================] - 4s 336us/step - loss: 0.0187 - acc: 1.0000\n",
            "6666/6666 [==============================] - 11s 2ms/step\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 32s 2ms/step - loss: 0.5795 - acc: 0.9742\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 7s 329us/step - loss: 0.4192 - acc: 1.0000\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 7s 339us/step - loss: 0.3669 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 6s 323us/step - loss: 0.3344 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 7s 329us/step - loss: 0.3095 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 7s 338us/step - loss: 0.2890 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 7s 337us/step - loss: 0.2710 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 7s 336us/step - loss: 0.2552 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 7s 345us/step - loss: 0.2406 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 7s 336us/step - loss: 0.2270 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
              "             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7f8a292ba9b0>,\n",
              "             iid='warn', n_jobs=None,\n",
              "             param_grid={'activation': ['softmax', 'relu', 'tanh', 'sigmoid',\n",
              "                                        'linear']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsTUbpHykyKA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "78a1296a-1775-40a2-f83f-9c158f9d079c"
      },
      "source": [
        "print(clf.best_score_,clf.best_params_)\n",
        "means=clf.cv_results_['mean_test_score']\n",
        "parameters=clf.cv_results_['params']\n",
        "for mean,parameter in zip(means,parameters):\n",
        "  print(mean,parameter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0 {'activation': 'softmax'}\n",
            "1.0 {'activation': 'softmax'}\n",
            "1.0 {'activation': 'relu'}\n",
            "1.0 {'activation': 'tanh'}\n",
            "1.0 {'activation': 'sigmoid'}\n",
            "1.0 {'activation': 'linear'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrMLaKglaN4X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBkvqqiJaaeg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDo0pgNTayW-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}